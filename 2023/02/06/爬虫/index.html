<!DOCTYPE html>

<html lang="en">

<head>
    
    <title>爬虫 - czw</title>
    <meta charset="UTF-8">
    <meta name="keywords" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">
    
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <meta name="description" content="BUG123456789101112ValueError: check_hostname requires server_hostname在用翻墙访问页面时遇到这个bug，原因是翻墙的话会用到代理，这时就需要指定proxies代理参数才可以。即proxies &#x3D; &amp;#123;&amp;#x27;http&amp;#x27;: &amp;#x27;http:&#x2F;&#x2F;localhost:7890&amp;#x27;, &amp;#x27;htt">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫">
<meta property="og:url" content="http://example.com/2023/02/06/%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="czw">
<meta property="og:description" content="BUG123456789101112ValueError: check_hostname requires server_hostname在用翻墙访问页面时遇到这个bug，原因是翻墙的话会用到代理，这时就需要指定proxies代理参数才可以。即proxies &#x3D; &amp;#123;&amp;#x27;http&amp;#x27;: &amp;#x27;http:&#x2F;&#x2F;localhost:7890&amp;#x27;, &amp;#x27;htt">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="c:\Users\86134\AppData\Roaming\Typora\typora-user-images\image-20230116152046257.png">
<meta property="article:published_time" content="2023-02-06T09:25:23.939Z">
<meta property="article:modified_time" content="2023-02-06T09:23:06.676Z">
<meta property="article:author" content="czw">
<meta property="article:tag" content="code">
<meta property="article:tag" content="知识分享">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:\Users\86134\AppData\Roaming\Typora\typora-user-images\image-20230116152046257.png">
    
<link rel="stylesheet" href="/lib/fancybox/fancybox.css">
<link rel="stylesheet" href="/lib/mdui_043tiny/mdui.css">


    <link rel="stylesheet" href="/lib/iconfont/iconfont.css?v=1675675995141">
    
    <link rel="stylesheet" href="/css/style.css?v=1675675995141">

    
        
            <link rel="stylesheet" href="/custom.css?v=1675675995141">
        
    
<meta name="generator" content="Hexo 6.3.0"></head>

<body class="mdui-drawer-body-left">
    <div id="nexmoe-background">
        <div class="nexmoe-bg" style="background-image: url(/pictures/a1.jpg)"></div>
        <div class="mdui-appbar mdui-shadow-0">
            <div class="mdui-toolbar">
                <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon mdui-ripple"><i class="mdui-icon nexmoefont icon-menu"></i></a>
                <div class="mdui-toolbar-spacer"></div>
                <!--<a href="javascript:;" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">search</i></a>-->
                <a href="/" title="czw" class="mdui-btn mdui-btn-icon"><img src="https://avatars.githubusercontent.com/u/88929041?s=400&amp;u=59bb1180edea8ab8a6918f927a2640ae391b8d15&amp;v=4" alt="czw"></a>
            </div>
        </div>
    </div>
    <div id="nexmoe-header">
        <div class="nexmoe-drawer mdui-drawer" id="drawer">
    <div class="nexmoe-avatar mdui-ripple">
        <a href="/" title="czw">
            <img src="https://avatars.githubusercontent.com/u/88929041?s=400&amp;u=59bb1180edea8ab8a6918f927a2640ae391b8d15&amp;v=4" alt="czw" alt="czw">
        </a>
    </div>
    <div class="nexmoe-count">
        <div><span>Articles</span>15</div>
        <div><span>Tags</span>6</div>
        <div><span>Categories</span>7</div>
    </div>
    <div class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}">
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/" title="回到首页">
            <i class="mdui-list-item-icon nexmoefont icon-home"></i>
            <div class="mdui-list-item-content">
                回到首页
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/archives.html" title="文章归档">
            <i class="mdui-list-item-icon nexmoefont icon-container"></i>
            <div class="mdui-list-item-content">
                文章归档
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/PY.html" title="我的朋友">
            <i class="mdui-list-item-icon nexmoefont icon-unorderedlist"></i>
            <div class="mdui-list-item-content">
                我的朋友
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/donate.html" title="给我赞助">
            <i class="mdui-list-item-icon nexmoefont icon-coffee"></i>
            <div class="mdui-list-item-content">
                给我赞助
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/about.html" title="关于博主">
            <i class="mdui-list-item-icon nexmoefont icon-info-circle"></i>
            <div class="mdui-list-item-content">
                关于博主
            </div>
        </a>
        
    </div>
    <aside id="nexmoe-sidebar">
    
        
        
        <div class="nexmoe-widget-wrap">
	<div class="nexmoe-widget nexmoe-social">
		<a
			class="mdui-ripple"
			href="https://github.com/black-da"
			target="_blank"
			mdui-tooltip="{content: 'GitHub'}"
			style="
				color: rgb(25, 23, 23);
				background-color: rgba(25, 23, 23, .1);
			"
		>
			<i
				class="nexmoefont icon-github"
			></i> </a
		><a
			class="mdui-ripple"
			href="/wechat.jpg"
			target="_blank"
			mdui-tooltip="{content: '微信'}"
			style="
				color: rgb(100, 230, 34);
				background-color: rgba(247, 132, 34, .1);
			"
		>
			<i
				class="nexmoefont icon-wechat-fill"
			></i> </a
		>
	</div>
</div>

    
        
        
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">Categories</h3>
    <div class="nexmoe-widget">

      <ul class="category-list">

        


        

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/code/">code</a>
          <span class="category-list-count">13</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/code/python/django/">django</a>
          <span class="category-list-count">2</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/code/python/">python</a>
          <span class="category-list-count">12</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/test/">test</a>
          <span class="category-list-count">1</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/code/windows/">windows</a>
          <span class="category-list-count">1</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/教程/">教程</a>
          <span class="category-list-count">1</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/code/python/爬虫/">爬虫</a>
          <span class="category-list-count">1</span>
        </li>

        
      </ul>

    </div>
  </div>


    
        
        
  <div class="nexmoe-widget-wrap">
    <div id="randomtagcloud" class="nexmoe-widget tagcloud nexmoe-rainbow">
      <a href="/tags/Games/" style="font-size: 10px;">Games</a> <a href="/tags/code/" style="font-size: 20px;">code</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/system/" style="font-size: 10px;">system</a> <a href="/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/" style="font-size: 15px;">知识分享</a>
    </div>
    
      <script>
        var maxTagcloud = parseInt(17);
        var tags_length = parseInt(6);
        var tags_arr = [];
        for(var i = 0; i < tags_length; i++){
          tags_arr.push(i);
        }
        tags_arr.sort(function (l, r) {
          return Math.random() > 0.5 ? -1 : 1;
        });
        tags_arr = tags_arr.slice(0, maxTagcloud < tags_length ? tags_length - maxTagcloud : 0);
        for(var tag_i = 0; tag_i < tags_arr.length; tag_i++){
          document.getElementById("randomtagcloud").children[tags_arr[tag_i]].style.display = 'none';
        }
      </script>
    
  </div>

    
        
        
        
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">Archive</h3>
    <div class="nexmoe-widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/">2023</a><span class="archive-list-count">15</span></li></ul>
    </div>
  </div>



    
        
        
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">Recent Posts</h3>
    <div class="nexmoe-widget">
      <ul>
        
          <li>
            <a href="/2023/02/06/%E7%88%AC%E8%99%AB/">爬虫</a>
          </li>
        
          <li>
            <a href="/2023/02/06/Django%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C/">Django数据库操作</a>
          </li>
        
          <li>
            <a href="/2023/02/05/Django%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%90%AD%E5%BB%BA/">Django项目的搭建和配置</a>
          </li>
        
          <li>
            <a href="/2023/02/05/%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E7%9A%84%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%9C%E7%94%A8/">环境变量的配置和作用</a>
          </li>
        
          <li>
            <a href="/2023/02/05/Python%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0/">Python内置函数</a>
          </li>
        
      </ul>
    </div>
  </div>

    
        
        <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-link">
		<ul>
        
            <li>
                <a href="https://blog.csdn.net/weixin_52709752?type=blog" target="_blank" >
                    <img src="/pictures/CSDN.png" alt="CSDN"></img>
                    <p>CSDN</p>
                </a>
            </li>
        
            <li>
                <a href="https://leetcode.cn/u/happy-svvartzu16/" target="_blank" >
                    <img src="/pictures/leetcode.png" alt="Leetcode"></img>
                    <p>Leetcode</p>
                </a>
            </li>
        
		</ul>
    </div>
</div>
<style>
.nexmoe-widget-wrap .nexmoe-link ul li a {
    text-align : center;
}
.nexmoe-widget-wrap .nexmoe-link ul li a img {
    max-width : 100%;
}
.nexmoe-widget-wrap .nexmoe-link ul li a p {
    margin: 10px 0;
}
</style>

    
</aside>
    <div class="nexmoe-copyright">
        &copy; 2023 czw
        Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
        & <a href="https://github.com/theme-nexmoe/hexo-theme-nexmoe" target="_blank">Nexmoe</a>
        
    </div>
</div><!-- .nexmoe-drawer -->
    </div>
    <div id="nexmoe-content">
        <div class="nexmoe-primary">
            <div class="nexmoe-post">

  <article>
    
        <div class="nexmoe-post-cover absolute" style="padding-top: 38.333333333333336%;"> 
            <img src="/pictures/a4.jpg" alt="爬虫" loading="lazy">
            <h1>爬虫</h1>
        </div>
    
    
    <div class="nexmoe-post-meta">
    <div class="nexmoe-rainbow">
        <a class="nexmoefont icon-calendar-fill">2023年02月06日</a>
        
            <a class="nexmoefont icon-appstore-fill -link" href="/categories/code/">code</a><a class="nexmoefont icon-appstore-fill -link" href="/categories/code/python/">python</a><a class="nexmoefont icon-appstore-fill -link" href="/categories/code/python/%E7%88%AC%E8%99%AB/">爬虫</a>
        
        
    </div>
    
    
    
    
    
</div>

    <h2 id="BUG"><a href="#BUG" class="headerlink" title="BUG"></a>BUG</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ValueError: check_hostname requires server_hostname</span><br><span class="line">在用翻墙访问页面时遇到这个bug，原因是翻墙的话会用到代理，这时就需要指定proxies代理参数才可以。</span><br><span class="line">即proxies = &#123;<span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://localhost:7890&#x27;</span>, <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;http://localhost:7890&#x27;</span>&#125;，端口号是启用代理的端口号</span><br><span class="line">然后用res = requests.get(url, proxies=proxies)就可以正常访问了</span><br><span class="line">在遇到其他需要访问网址的页面都要加上proxies参数，比如调用谷歌翻译API进行翻译时，实例化对象时用client = Translate(proxies=&#123;<span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;http://localhost:10809&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">如果不用翻墙的话，直接把代理关了问题也可以解决了</span><br><span class="line"></span><br><span class="line">上面的处理，在遇到其他去要使用代理去访问网站的问题，比如通过某个包访问谷歌翻译时要访问网址还是会报一样的错误，除非手动加代理。下面的方法更普适一些</span><br><span class="line">其实产生这个问题的根本原因是urllib3版本太高，新版的urllib3修改了一些东西，所以会报错。下面的解决办法是降低urlib3的版本。</span><br><span class="line">在命令行运行pip install urllib3==<span class="number">1.25</span><span class="number">.11</span></span><br><span class="line">如果失败就运行pip install urllib3==<span class="number">1.25</span><span class="number">.11</span> -i http://pypi.douban.com/simple --trusted-host pypi.douban.com</span><br></pre></td></tr></table></figure>



<h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><ul>
<li><p>XHR可以看到ajax请求 。一般直接朝url发请求返回的页面里如果没有想要的数据，那么想要的数据就是用ajax朝后端请求的，就要通过查看XHR来确定自己想要的资料在那个请求里</p>
</li>
<li><p>pip install -r README.md</p>
</li>
<li><p>要是ip被禁了，可以开vpn换个代理。也可以手动进去几次，干扰机器识别</p>
</li>
<li><p>有些网站访问不同的页面cookies是有变化的，针对这种情况可以先用session登陆一次拿到cookie，之后再用拿到的cookie继续发请求</p>
</li>
<li><p>遇到<code>module not found</code>的问题，可能是模块的版本不匹配的原因，直接把对应的模块pip uninstall了，再pip  install 一下就好了</p>
</li>
<li><p>&#96;&#96;&#96;python<br>用Python将网页上抓取下来的base64字符串转换成图片的形式保存到本地<br>base64_str &#x3D; ‘data:image&#x2F;jpg;base64,&#x2F;9j&#x2F;4AAQSkZJRgABAQAAAQABAAD&#x2F;2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8’<br>res &#x3D; base64_str.split(“,”)[1]<br>img_base64_decode &#x3D; base64.b64decode(res)<br>with open(‘a.jpg’, ‘wb’) as img:<br>img.write(img_base64_decode)<br>把图片转为base64二进制格式存储：<br>with open(‘0.jpg’, ‘rb’) as f:<br>content &#x3D; f.read()<br>print(base64.b64encode(content))</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* ```python</span><br><span class="line">  将页面滚动到最后的js</span><br><span class="line">  // 定义函数</span><br><span class="line">  (function page_scroll() &#123;</span><br><span class="line">      var i = 1</span><br><span class="line">      var element = document.documentElement</span><br><span class="line">      element.scrollTop = 0;  // 不管他在哪里，都让他先回到最上面</span><br><span class="line">   </span><br><span class="line">      // 设置定时器，时间即为滚动速度</span><br><span class="line">      function main() &#123;</span><br><span class="line">          if (element.scrollTop + element.clientHeight == element.scrollHeight) &#123;</span><br><span class="line">              clearInterval(interval)</span><br><span class="line">              console.log(&#x27;已经到底部了&#x27;)</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">              element.scrollTop += 300;</span><br><span class="line">              console.log(i);</span><br><span class="line">              i += 1;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      // 定义ID</span><br><span class="line">      interval = setInterval(main, 300)</span><br><span class="line">  &#125;) ()</span><br><span class="line">  # page_scroll()</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="爬虫介绍"><a href="#爬虫介绍" class="headerlink" title="爬虫介绍"></a>爬虫介绍</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> 爬虫：网络蜘蛛</span><br><span class="line"><span class="number">2</span> 爬虫本质：模拟浏览器发送请求（requests，selenium）-&gt;下载网页代码-&gt;只提取有用的数据（bs4，xpath，re）-&gt;存放于数据库或文件中（文件，excel，mysql，redis，mongodb）</span><br><span class="line"><span class="number">3</span> 发送请求：请求地址（浏览器调试，抓包工具），请求头，请求体，请求方法</span><br><span class="line"><span class="number">4</span> 拿到响应：拿到响应体（json格式，xml格式，html格式（bs4,xpath），加密的未知格式(需要解密)）</span><br><span class="line"><span class="number">5</span> 入库：Mongodb（json格式数据）</span><br><span class="line"><span class="number">6</span> 性能高一些（多线程，多进程，协程），只针对与python语言的cpython解释器（GIL：同一时刻只能由一个线程在执行）</span><br><span class="line">	-io密集型：用线程</span><br><span class="line">    -计算密集型：用进程</span><br></pre></td></tr></table></figure>



<h2 id="requests模块使用"><a href="#requests模块使用" class="headerlink" title="requests模块使用"></a>requests模块使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> 安装：pip3 install requests</span><br><span class="line"><span class="number">2</span> 图片防盗链：referer <span class="comment">#存的是你是从什么网页跳转到当前网页的，如果你之前的网页不是这个服务器的网页，那么就不会给你数据。referer可以在网页的headers里找到</span></span><br><span class="line"><span class="number">3</span> 代码</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 发送get请求</span></span><br><span class="line"><span class="comment"># res是python的对象，对象里有响应头，响应体。。。。</span></span><br><span class="line"><span class="comment"># header = &#123;</span></span><br><span class="line"><span class="comment">#     &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;referer&#x27;: &#x27;https://www.mzitu.com/225078/2&#x27;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"><span class="comment"># # res = requests.get(&#x27;https://www.mzitu.com/&#x27;, headers=header)</span></span><br><span class="line"><span class="comment"># # print(res.text) # 网页的文本数据</span></span><br><span class="line"><span class="comment"># res1 = requests.get(&#x27;https://i3.mmzztt.com/2020/03/14a02.jpg&#x27;, headers=header)</span></span><br><span class="line"><span class="comment"># # print(res1.content)  # 二进制内容，通常用于音频或图片的下载</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># with open(&#x27;a.jpg&#x27;, &#x27;wb&#x27;)as f:</span></span><br><span class="line"><span class="comment">#     for line in res1.iter_content():</span></span><br><span class="line"><span class="comment">#         f.write(line)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 请求地址中携带数据(两种方式，推荐第二种),在发送http请求时，通常会对url进行编码，如果url中存在中文的话，可能发送过去就变成乱码了，所以需要注意编码</span></span><br><span class="line">第<span class="number">1</span>种 手动编码解码</span><br><span class="line"><span class="comment"># from urllib.parse import urlencode,unquote  # url的编码和解码</span></span><br><span class="line"><span class="comment"># print(unquote(&#x27;%E7%BE%8E%E5%A5%B3&#x27;)) # 打印&#x27;美女&#x27;</span></span><br><span class="line"><span class="comment"># header = &#123;</span></span><br><span class="line"><span class="comment">#     &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27;,</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"><span class="comment"># # res=requests.get(&#x27;https://www.baidu.com/s?wd=美女&#x27;,headers=header)# 这里的美女发送过去可能因为中文编码的问题变成乱码，所以需要特殊处理一下</span></span><br><span class="line"><span class="comment"># res=requests.get(&#x27;https://www.baidu.com/s&#x27;,headers=header,params=&#123;&#x27;wd&#x27;:&#x27;美女&#x27;&#125;)</span></span><br><span class="line"><span class="comment"># print(res.url)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 请求带cookie(两种方式)</span></span><br><span class="line"><span class="comment"># 方式一，在header中放</span></span><br><span class="line"><span class="comment"># header = &#123;</span></span><br><span class="line"><span class="comment">#     &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;cookie&#x27;:&#x27;key=asdfasdfasdfsdfsaasdf;key2=asdfasdf;key3=asdfasdf&#x27;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"><span class="comment"># res=requests.get(&#x27;http://127.0.0.1:8000/index/&#x27;,headers=header)</span></span><br><span class="line"><span class="comment"># 方式二：</span></span><br><span class="line"><span class="comment"># header = &#123;</span></span><br><span class="line"><span class="comment">#     &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27;,</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"><span class="comment"># # cookies是一个字典或者CookieJar对象</span></span><br><span class="line"><span class="comment"># res=requests.get(&#x27;http://127.0.0.1:8000/index/&#x27;,headers=header,cookies=&#123;&#x27;key&#x27;:&#x27;asdfasdf&#x27;&#125;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 发送post请求，携带数据（urlencoded和json）</span></span><br><span class="line"><span class="comment"># res=requests.post(&#x27;http://127.0.0.1:8000/index/&#x27;,data=&#123;&#x27;name&#x27;:&#x27;lqz&#x27;&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># res=requests.post(&#x27;http://127.0.0.1:8000/index/&#x27;,json=&#123;&#x27;age&#x27;:1,&#125;,)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5 自动携带cookie</span></span><br><span class="line"><span class="comment"># session=requests.session()</span></span><br><span class="line"><span class="comment"># res=session.post(&#x27;http://127.0.0.1:8000/index/&#x27;)  # 假设这个请求登录了</span></span><br><span class="line"><span class="comment"># res1=session.get(&#x27;http://127.0.0.1:8000/order/&#x27;)  # 现在不需要手动带cookie，session会帮咱处理</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6 response对象，就是前面的res</span></span><br><span class="line"><span class="comment"># respone=requests.post(&#x27;http://127.0.0.1:8000/index/&#x27;,data=&#123;&#x27;name&#x27;:&#x27;lqz&#x27;&#125;)</span></span><br><span class="line"><span class="comment"># # print(respone.text)  # 响应的文本</span></span><br><span class="line"><span class="comment"># print(respone.content)  # 响应体的二进制</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># print(respone.status_code)  # 响应状态码</span></span><br><span class="line"><span class="comment"># print(respone.headers)    # 响应头</span></span><br><span class="line"><span class="comment"># print(respone.cookies)   # cookie</span></span><br><span class="line"><span class="comment"># print(respone.cookies.get_dict()) #  把cookie转成字典</span></span><br><span class="line"><span class="comment"># print(respone.cookies.items())  # key和value[(key, value), ]</span></span><br><span class="line"><span class="comment"># print(respone.url)        # 请求的url</span></span><br><span class="line"><span class="comment"># print(respone.history)   #[]放重定向之前的地址</span></span><br><span class="line"><span class="comment"># print(respone.encoding)  # 响应的编码方式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># respone.iter_content()  # 图片，视频，大文件，一点一点循环取出来</span></span><br><span class="line"><span class="comment"># for line in respone.iter_content():</span></span><br><span class="line"><span class="comment">#     f.write(line)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7 编码问题</span></span><br><span class="line"><span class="comment"># res=requests.get(&#x27;http://www.autohome.com/news&#x27;)</span></span><br><span class="line"><span class="comment"># # 一旦打印出来出现乱码问题，那就是编码出错了</span></span><br><span class="line"><span class="comment"># # 方式一</span></span><br><span class="line"><span class="comment"># # res.encoding=&#x27;gb2312&#x27;</span></span><br><span class="line"><span class="comment"># # 方式二</span></span><br><span class="line"><span class="comment"># res.encoding=res.apparent_encoding，会帮你自动获取页面的编码方式</span></span><br><span class="line"><span class="comment"># print(res.text)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 8 解析json</span></span><br><span class="line"><span class="comment"># import json</span></span><br><span class="line"><span class="comment"># respone=requests.post(&#x27;http://127.0.0.1:8000/index/&#x27;,data=&#123;&#x27;name&#x27;:&#x27;lqz&#x27;&#125;)</span></span><br><span class="line"><span class="comment"># # print(json.loads(respone.text))</span></span><br><span class="line"><span class="comment"># print(respone.json())  # 相当于上面那句话</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9 高级用法：使用代理。就是爬虫先向代理的ip发请求，再利用代理的ip去向目标网址发请求，这样目标网址封ip封的也是代理ip，而不是我的ip</span></span><br><span class="line"><span class="comment"># respone=requests.get(&#x27;http://127.0.0.1:8000/index/&#x27;,proxies=&#123;&#x27;http&#x27;:&#x27;代理的地址和端口号&#x27;,&#125;)</span></span><br><span class="line">代理可以去百度</span><br><span class="line"><span class="comment"># 代理，免费代理，收费代理花钱买</span></span><br><span class="line"><span class="comment"># 代理池：列表放了一堆代理ip，每次随机取一个，再发请求就不会封ip了</span></span><br><span class="line"><span class="comment"># 高匿和透明代理？如果使用高匿代理，后端无论如何拿不到你的ip，使用透明，后端能够拿到你的ip</span></span><br><span class="line"><span class="comment"># 后端如何拿到透明代理的ip，  后端：X-Forwarded-For</span></span><br><span class="line"><span class="comment"># respone=requests.get(&#x27;https://www.baidu.com/&#x27;,proxies=&#123;&#x27;http&#x27;:&#x27;27.46.20.226:8888&#x27;,&#125;)</span></span><br><span class="line"><span class="comment"># print(respone.text)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 10 超时设置</span></span><br><span class="line"><span class="comment"># import requests</span></span><br><span class="line"><span class="comment"># respone=requests.get(&#x27;https://www.baidu.com&#x27;,</span></span><br><span class="line"><span class="comment">#                      timeout=0.0001)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 13 异常处理</span></span><br><span class="line"><span class="comment"># try:</span></span><br><span class="line"><span class="comment">#     r=requests.get(&#x27;http://www.baidu.com&#x27;,timeout=0.00001)</span></span><br><span class="line"><span class="comment"># except Exception as e:</span></span><br><span class="line"><span class="comment">#     print(e)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 14 上传文件</span></span><br><span class="line"><span class="comment"># res=requests.post(&#x27;http://127.0.0.1:8000/index/&#x27;,files=&#123;&#x27;myfile&#x27;:open(&#x27;a.jpg&#x27;,&#x27;rb&#x27;)&#125;)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="模拟登陆某网站"><a href="#模拟登陆某网站" class="headerlink" title="模拟登陆某网站"></a>模拟登陆某网站</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#http://www.aa7a.cn/</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">session=requests.session()</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;username&#x27;</span>: <span class="string">&#x27;616564099@qq.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;password&#x27;</span>: <span class="string">&#x27;lqz123&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;captcha&#x27;</span>: <span class="string">&#x27;zdu4&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;remember&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;ref&#x27;</span>: <span class="string">&#x27;http://www.aa7a.cn/user.php?act=logout&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;act&#x27;</span>: <span class="string">&#x27;act_login&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line">rest = session.post(<span class="string">&#x27;http://www.aa7a.cn/user.php&#x27;</span>,data=data)</span><br><span class="line"><span class="built_in">print</span>(rest.text)</span><br><span class="line"><span class="comment"># 拿到cookie</span></span><br><span class="line">cookie=rest.cookies</span><br><span class="line"><span class="built_in">print</span>(cookie)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 携带着cookies，表示登录了，页面中会有我们的用户信息616564099@qq.com</span></span><br><span class="line">rest1=session.get(<span class="string">&#x27;http://www.aa7a.cn/index.php&#x27;</span>)</span><br><span class="line"><span class="comment"># rest1=requests.get(&#x27;http://www.aa7a.cn/index.php&#x27;)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;616564099@qq.com&#x27;</span> <span class="keyword">in</span> rest1.text)</span><br></pre></td></tr></table></figure>



<h2 id="爬取梨视频-re解析文档"><a href="#爬取梨视频-re解析文档" class="headerlink" title="爬取梨视频(re解析文档)"></a>爬取梨视频(re解析文档)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">这网站改了，再网页里已经找不到mp4的信息了</span><br><span class="line"><span class="comment">#https://www.pearvideo.com/</span></span><br><span class="line"><span class="comment"># 爬取梨视频</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">res=requests.get(<span class="string">&#x27;https://www.pearvideo.com/category_loading.jsp?reqType=5&amp;categoryId=1&amp;start=0&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(res.text)</span></span><br><span class="line">re_video=<span class="string">&#x27;&lt;a href=&quot;(.*?)&quot; class=&quot;vervideo-lilink actplay&quot;&gt;&#x27;</span></span><br><span class="line">video_urls=re.findall(re_video,res.text)</span><br><span class="line"><span class="comment"># https://www.pearvideo.com/</span></span><br><span class="line"><span class="comment"># print(video_urls)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> video <span class="keyword">in</span> video_urls:</span><br><span class="line">    url=<span class="string">&#x27;https://www.pearvideo.com/&#x27;</span>+video</span><br><span class="line">    <span class="built_in">print</span>(url)</span><br><span class="line">    <span class="comment"># 向视频详情发送get请求</span></span><br><span class="line">    res_video=requests.get(url)</span><br><span class="line">    <span class="comment"># print(res_video.text)</span></span><br><span class="line">    <span class="comment"># break</span></span><br><span class="line">    re_video_mp4=<span class="string">&#x27;hdUrl=&quot;&quot;,sdUrl=&quot;&quot;,ldUrl=&quot;&quot;,srcUrl=&quot;(.*?)&quot;,vdoUrl=srcUrl,skinRes&#x27;</span></span><br><span class="line">    video_url=re.findall(re_video_mp4,res_video.text)[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(video_url)</span><br><span class="line">    video_name=video_url.rsplit(<span class="string">&#x27;/&#x27;</span>,<span class="number">1</span>)[-<span class="number">1</span>]</span><br><span class="line">    <span class="built_in">print</span>(video_name)</span><br><span class="line">    res_video_content=requests.get(video_url)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(video_name,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> res_video_content.iter_content():</span><br><span class="line">            f.write(line)</span><br></pre></td></tr></table></figure>

<h2 id="bs4的使用"><a href="#bs4的使用" class="headerlink" title="bs4的使用"></a>bs4的使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">本质上是一个文本解析器，可以解析html和xml格式的文档</span><br><span class="line">使用前导入：</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(document, parser) <span class="comment"># 第一个参数是要解析的文档，第二个是解析器</span></span><br><span class="line"><span class="comment"># 解析器一般用&#x27;html.parser&#x27;(Python内置的，直接用);</span></span><br><span class="line"><span class="comment"># 或&#x27;lxml&#x27;,需要安装：pip install lxml</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索文档树</span></span><br><span class="line"><span class="comment"># find()  # 只返回找到的第一个</span></span><br><span class="line"><span class="comment"># find_all() # 找到的所有</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 五种过滤器: 字符串、正则表达式、列表、True、方法</span></span><br><span class="line"><span class="comment"># 字符串过滤，通过字符串过滤内容</span></span><br><span class="line"><span class="comment"># a=soup.find(name=&#x27;a&#x27;) 标签名</span></span><br><span class="line"><span class="comment"># res=soup.find(id=&#x27;my_p&#x27;) id</span></span><br><span class="line"><span class="comment"># res=soup.find(class_=&#x27;story&#x27;) class，避免关键字冲突，故用class_</span></span><br><span class="line"><span class="comment"># res=soup.find(href=&#x27;http://example.com/elsie&#x27;) href</span></span><br><span class="line"><span class="comment"># res=soup.find(id=&#x27;my_p&#x27;， name=&#x27;p&#x27;) id and name，是and的关系</span></span><br><span class="line"><span class="comment"># res=soup.find(attrs=&#123;&#x27;id&#x27;:&#x27;my_p&#x27;&#125;) 用字典的方式也可</span></span><br><span class="line"><span class="comment"># res=soup.find(attrs=&#123;&#x27;class&#x27;:&#x27;story&#x27;&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则表达式</span></span><br><span class="line"><span class="comment"># import re</span></span><br><span class="line"><span class="comment"># # re_b=re.compile(&#x27;^b&#x27;)</span></span><br><span class="line"><span class="comment"># res=soup.find(name=re_b)</span></span><br><span class="line"><span class="comment"># # res=soup.find_all(name=re_b)</span></span><br><span class="line"><span class="comment"># res=soup.find_all(id=re.compile(&#x27;^l&#x27;))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 列表</span></span><br><span class="line"><span class="comment"># res=soup.find_all(name=[&#x27;body&#x27;,&#x27;b&#x27;])</span></span><br><span class="line"><span class="comment"># res=soup.find_all(class_=[&#x27;sister&#x27;,&#x27;title&#x27;])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># True和false</span></span><br><span class="line"><span class="comment"># res=soup.find_all(name=True)</span></span><br><span class="line"><span class="comment"># res=soup.find_all(id=True)</span></span><br><span class="line"><span class="comment"># res=soup.find_all(id=False)</span></span><br><span class="line"><span class="comment"># res=soup.find_all(href=True)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># css选择</span></span><br><span class="line"><span class="comment"># ret=soup.select(&#x27;#my_p&#x27;) 返回的是个列表</span></span><br><span class="line"><span class="comment"># https://www.w3school.com.cn/cssref/css_selectors.asp</span></span><br><span class="line"><span class="comment"># ret=soup.select(&#x27;body p&#x27;)  # 子子孙孙</span></span><br><span class="line"><span class="comment"># ret=soup.select(&#x27;body&gt;p&#x27;)  # 直接子节点（儿子）</span></span><br><span class="line"><span class="comment"># ret=soup.select(&#x27;body&gt;p&#x27;)[0].text  # 直接子节点（儿子）</span></span><br><span class="line"><span class="comment"># # ret=soup.select(&#x27;body&gt;p&#x27;)[0].a.find() 可以混搭使用的，都是Tag对象</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="代理池"><a href="#代理池" class="headerlink" title="代理池"></a>代理池</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">就是写一个爬虫去全网爬代理ip地址，然后保存在数据库</span><br><span class="line">再开一个web服务，每次有请求就随机从数据库取一个ip地址返回</span><br><span class="line"><span class="comment"># github，下载免费代理池开源代码（建议读一下别人的代码）</span></span><br><span class="line"><span class="comment"># git clone git@github.com:jhao104/proxy_pool.git</span></span><br><span class="line"><span class="comment"># pycharm打开，修改配置文件（reids地址修改）</span></span><br><span class="line"><span class="comment"># 启动爬虫：</span></span><br><span class="line">python proxyPool.py schedule</span><br><span class="line"><span class="comment"># 启动服务：</span></span><br><span class="line">python3 proxyPool.py server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机获取一个代理</span></span><br><span class="line">requests.get(<span class="string">&quot;http://127.0.0.1:5010/get/&quot;</span>).json()</span><br><span class="line"><span class="comment">#删除一个代理</span></span><br><span class="line">requests.get(<span class="string">&quot;http://127.0.0.1:5010/delete/?proxy=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(proxy))</span><br></pre></td></tr></table></figure>

<h2 id="验证码破解之-打码平台介绍"><a href="#验证码破解之-打码平台介绍" class="headerlink" title="验证码破解之-打码平台介绍"></a>验证码破解之-打码平台介绍</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 验证码破解 图像处理</span></span><br><span class="line"><span class="comment"># 2 专业打码平台，破解验证码（收费）</span></span><br><span class="line"><span class="comment"># 申请超级鹰，注册</span></span><br><span class="line"><span class="comment"># 登录，下载sdk（代码如下），填入用户名密码，软件id</span></span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding:utf-8</span></span><br></pre></td></tr></table></figure>

<h2 id="xpath选择器使用"><a href="#xpath选择器使用" class="headerlink" title="xpath选择器使用"></a>xpath选择器使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查找标签的三种方式，bs4的find，     css选择器，xpath查找（后2种时许多语言通用的）</span></span><br><span class="line"><span class="comment"># xpath: XPath 是一门在 XML 文档中查找信息的语言</span></span><br><span class="line"><span class="comment"># / :从当前节点的直接子节点中选取。一开始默认是从根节点开始找的</span></span><br><span class="line"><span class="comment"># // :从当前节点的子子孙孙中递归选取</span></span><br><span class="line"><span class="comment"># /@属性名</span></span><br><span class="line"><span class="comment"># /text()</span></span><br><span class="line"><span class="comment"># xpath路径可以直接从浏览器复制</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html=etree.HTML(doc)</span><br><span class="line">html=etree.parse(<span class="string">&#x27;search.html&#x27;</span>,etree.HTMLParser())</span><br><span class="line"><span class="comment"># 1 所有节点</span></span><br><span class="line"><span class="comment"># a=html.xpath(&#x27;//*&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 指定节点（结果为列表）</span></span><br><span class="line"><span class="comment"># a=html.xpath(&#x27;//head&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 子节点，子孙节点</span></span><br><span class="line"><span class="comment"># a=html.xpath(&#x27;//div/a&#x27;)</span></span><br><span class="line"><span class="comment"># a=html.xpath(&#x27;//body//a&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 属性匹配</span></span><br><span class="line"><span class="comment"># a=html.xpath(&#x27;//body//a[@href=&quot;image1.html&quot;]&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5 文本获取(重要)  /text() 取当前标签的文本</span></span><br><span class="line"><span class="comment"># a=html.xpath(&#x27;//body//a/text()&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6 属性获取  @href 取当前标签的属性</span></span><br><span class="line"><span class="comment"># a=html.xpath(&#x27;//body//a/@href&#x27;)</span></span><br><span class="line"><span class="comment"># # 获取某个属性 注意从1 开始取（不是从0）</span></span><br><span class="line"><span class="comment"># a=html.xpath(&#x27;//body//a[1]/@href&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7 属性多值匹配</span></span><br><span class="line"><span class="comment">#  a 标签有多个class类，直接匹配就不可以了，需要用contains。直接匹配需要所有写全所有的class才匹配得上</span></span><br><span class="line"><span class="comment"># a=html.xpath(&#x27;//body//a[@class=&quot;li&quot;]&#x27;) 当a只有一个class时可用</span></span><br><span class="line"><span class="comment"># a=html.xpath(&#x27;//body//a[contains(@class,&quot;li&quot;)]&#x27;) 当a除了li之外还有其他class时用</span></span><br><span class="line"><span class="comment"># a=html.xpath(&#x27;//body//a[contains(@class,&quot;li&quot;)]/text()&#x27;)</span></span><br><span class="line"><span class="comment"># 8 多属性匹配</span></span><br><span class="line"><span class="comment"># a=html.xpath(&#x27;//body//a[contains(@class,&quot;li&quot;) and @name=&quot;items&quot;]/text()&#x27;)</span></span><br></pre></td></tr></table></figure>

<h2 id="selenium使用"><a href="#selenium使用" class="headerlink" title="selenium使用"></a>selenium使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># selenium可以用代码操控浏览器，为了解决requests无法直接执行JavaScript代码的问题 </span></span><br><span class="line"><span class="comment"># pip3 install selenium</span></span><br><span class="line"><span class="comment"># 浏览器驱动:http://npm.taobao.org/mirrors/chromedriver/</span></span><br><span class="line"><span class="comment"># 驱动要跟浏览器版本对应</span></span><br><span class="line"><span class="comment"># 下载完解压就是个exe（不同平台的可执行文件）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># from selenium import webdriver</span></span><br><span class="line"><span class="comment"># import time</span></span><br><span class="line"><span class="comment"># # 指定使用跟那个驱动</span></span><br><span class="line"><span class="comment"># bro=webdriver.Chrome(executable_path=&#x27;./chromedriver.exe&#x27;) # 得到一个谷歌浏览器对象，</span></span><br><span class="line"><span class="comment"># time.sleep(2)</span></span><br><span class="line"><span class="comment"># bro.get(&#x27;https://www.baidu.com/&#x27;)  # 在地址栏里输入了百度</span></span><br><span class="line"><span class="comment"># time.sleep(2)</span></span><br><span class="line"><span class="comment"># print(bro.page_source)</span></span><br><span class="line"><span class="comment"># time.sleep(2)</span></span><br><span class="line"><span class="comment"># bro.close()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ##############选择器（find系列）</span></span><br><span class="line"><span class="comment"># ===============所有方法===================</span></span><br><span class="line"><span class="comment"># 1、find_element_by_id   # 通过id查找控件</span></span><br><span class="line"><span class="comment"># 2、find_element_by_link_text  # 通过a标签内容找</span></span><br><span class="line"><span class="comment"># 3、find_element_by_partial_link_text  # 通过a标签内容找，模糊匹配，比如输入一个‘登’就可找到内容为‘登陆’的a标签</span></span><br><span class="line"><span class="comment"># 4、find_element_by_tag_name   # 标签名</span></span><br><span class="line"><span class="comment"># 5、find_element_by_class_name  # 类名</span></span><br><span class="line"><span class="comment"># 6、find_element_by_name      # name属性</span></span><br><span class="line"><span class="comment"># 7、find_element_by_css_selector  # 通过css选择器</span></span><br><span class="line"><span class="comment"># 8、find_element_by_xpath       # 通过xpaht选择器</span></span><br><span class="line"><span class="comment"># 强调：</span></span><br><span class="line"><span class="comment"># 1、find_elements_by_xxx的形式是查找到多个元素，结果为列表</span></span><br><span class="line"></span><br><span class="line">find方法被弃用了，新方法是</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line">find_element(self, by=By.ID, value=‘id_name’)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取元素属性</span></span><br><span class="line"><span class="comment"># 重点</span></span><br><span class="line"><span class="comment"># tag.get_attribute(&#x27;href&#x27;)  # 找当前控件 的href属性对的值</span></span><br><span class="line"><span class="comment"># tag.text   # 获取文本内容</span></span><br><span class="line"><span class="comment"># 了解</span></span><br><span class="line"><span class="comment"># print(tag.location)  # 当前控件在页面位置</span></span><br><span class="line"><span class="comment"># print(tag.size)      #标签的大小</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">####无界面浏览器（phantomjs）</span></span><br><span class="line"><span class="comment">#谷歌浏览器支持不打开页面</span></span><br><span class="line"><span class="comment"># from selenium.webdriver.chrome.options import Options</span></span><br><span class="line"><span class="comment"># from selenium import webdriver</span></span><br><span class="line"><span class="comment"># chrome_options = Options()</span></span><br><span class="line"><span class="comment"># chrome_options.add_argument(&#x27;window-size=1920x3000&#x27;) #指定浏览器分辨率</span></span><br><span class="line"><span class="comment"># chrome_options.add_argument(&#x27;--disable-gpu&#x27;) #谷歌文档提到需要加上这个属性来规避bug</span></span><br><span class="line"><span class="comment"># chrome_options.add_argument(&#x27;--hide-scrollbars&#x27;) #隐藏滚动条, 应对一些特殊页面</span></span><br><span class="line"><span class="comment"># chrome_options.add_argument(&#x27;blink-settings=imagesEnabled=false&#x27;) #不加载图片, 提升速度</span></span><br><span class="line"><span class="comment"># chrome_options.add_argument(&#x27;--headless&#x27;) #浏览器不提供可视化页面. </span></span><br><span class="line"><span class="comment">#bro=webdriver.Chrome(chrome_options=chrome_options,executable_path=&#x27;./chromedriver.exe&#x27;)</span></span><br><span class="line"><span class="comment"># bro.get(&#x27;https://www.baidu.com/&#x27;)</span></span><br><span class="line"><span class="comment"># print(bro.page_source)</span></span><br><span class="line"><span class="comment"># bro.close()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######元素交互</span></span><br><span class="line"><span class="comment"># tag.send_keys()  # 往里面写内容</span></span><br><span class="line"><span class="comment"># tag.click()      # 点击控件</span></span><br><span class="line"><span class="comment"># tag.clear()      # 清空控件内容</span></span><br><span class="line"><span class="comment"># input_k.send_keys(Keys.ENTER) # 模拟键盘的回车键from selenium.webdriver.common.keys import Keys</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># bro=webdriver.Chrome(executable_path=&#x27;./chromedriver.exe&#x27;)</span></span><br><span class="line"><span class="comment"># bro.implicitly_wait(5)  # 隐式等待：找一个控件，如果控件没有加载出来，等待5s，5s内找到控件立马继续执行，5s后找不到就报错  对所有控件生效</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#####执行js代码</span></span><br><span class="line"><span class="comment"># bro.execute_script(&#x27;window.open()&#x27;)</span></span><br><span class="line"><span class="comment"># time.sleep(2)</span></span><br><span class="line"><span class="comment"># bro.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#####获取cookie</span></span><br><span class="line"><span class="comment"># bro.get_cookies()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### 如何把屏幕拉倒最后（js控制）</span></span><br><span class="line"><span class="comment"># bro.execute_script(&#x27;window.scrollTo(0,document.body.offsetHeight)&#x27;)</span></span><br></pre></td></tr></table></figure>

<h2 id="爬拉勾网职位信息-不同页面cookie不一样，得先拿到cookie"><a href="#爬拉勾网职位信息-不同页面cookie不一样，得先拿到cookie" class="headerlink" title="爬拉勾网职位信息(不同页面cookie不一样，得先拿到cookie)"></a>爬拉勾网职位信息(不同页面cookie不一样，得先拿到cookie)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#https://www.lagou.com/jobs/positionAjax.json?city=%E4%B8%8A%E6%B5%B7&amp;needAddtionalResult=false</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment">#实际要爬取的url</span></span><br><span class="line">url = <span class="string">&#x27;https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false&#x27;</span></span><br><span class="line"></span><br><span class="line">payload = &#123;</span><br><span class="line">    <span class="string">&#x27;first&#x27;</span>: <span class="string">&#x27;true&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;pn&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;kd&#x27;</span>: <span class="string">&#x27;python&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">header = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;https://www.lagou.com/jobs/list_python?labelWords=&amp;fromSearch=true&amp;suginput=&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;application/json, text/javascript, */*; q=0.01&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#原始的url</span></span><br><span class="line">urls =<span class="string">&#x27;https://www.lagou.com/jobs/list_python?labelWords=&amp;fromSearch=true&amp;suginput=&#x27;</span></span><br><span class="line"><span class="comment">#建立session</span></span><br><span class="line">s = requests.Session()</span><br><span class="line"><span class="comment"># 获取搜索页的cookies</span></span><br><span class="line">s.get(urls, headers=header, timeout=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 为此次获取的cookies</span></span><br><span class="line">cookie = s.cookies</span><br><span class="line"><span class="comment"># 获取此次文本</span></span><br><span class="line">response = s.post(url, data=payload, headers=header, cookies=cookie, timeout=<span class="number">5</span>).text</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="自动登录12306"><a href="#自动登录12306" class="headerlink" title="自动登录12306"></a>自动登录12306</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">破解验证码：先截整个屏，再根据控件的位置和尺寸把验证码部分截图下来，发送到打码平台超级鹰，再根据超级鹰返回的坐标用动作链去点击验证码上的图片即可</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打码平台的使用</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment">#pillow</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 引入超级鹰</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> chaojiying <span class="keyword">import</span> Chaojiying_Client</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> ActionChains</span><br><span class="line">bro=webdriver.Chrome(executable_path=<span class="string">&#x27;./chromedriver.exe&#x27;</span>)</span><br><span class="line">bro.implicitly_wait(<span class="number">10</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    bro.get(<span class="string">&#x27;https://kyfw.12306.cn/otn/resources/login.html&#x27;</span>)</span><br><span class="line">    bro.maximize_window()  <span class="comment"># 窗口最大化，全屏</span></span><br><span class="line">    button_z=bro.find_element_by_css_selector(<span class="string">&#x27;.login-hd-account a&#x27;</span>)</span><br><span class="line">    button_z.click()</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 截取整个屏幕</span></span><br><span class="line">    bro.save_screenshot(<span class="string">&#x27;./main.png&#x27;</span>)</span><br><span class="line">    <span class="comment"># 验证码的位置和大小</span></span><br><span class="line">    img_t=bro.find_element_by_id(<span class="string">&#x27;J-loginImg&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(img_t.size)</span><br><span class="line">    <span class="built_in">print</span>(img_t.location)</span><br><span class="line"></span><br><span class="line">    size=img_t.size</span><br><span class="line">    location=img_t.location</span><br><span class="line"></span><br><span class="line">    img_tu = (<span class="built_in">int</span>(location[<span class="string">&#x27;x&#x27;</span>]), <span class="built_in">int</span>(location[<span class="string">&#x27;y&#x27;</span>]), <span class="built_in">int</span>(location[<span class="string">&#x27;x&#x27;</span>] + size[<span class="string">&#x27;width&#x27;</span>]), <span class="built_in">int</span>(location[<span class="string">&#x27;y&#x27;</span>] + size[<span class="string">&#x27;height&#x27;</span>]))</span><br><span class="line">    <span class="comment"># # 抠出验证码</span></span><br><span class="line">    <span class="comment"># #打开</span></span><br><span class="line">    img = Image.<span class="built_in">open</span>(<span class="string">&#x27;./main.png&#x27;</span>)</span><br><span class="line">    <span class="comment"># 抠图</span></span><br><span class="line">    fram = img.crop(img_tu)</span><br><span class="line">    <span class="comment"># 截出来的小图</span></span><br><span class="line">    fram.save(<span class="string">&#x27;code.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用超级鹰破解</span></span><br><span class="line">    chaojiying = Chaojiying_Client(<span class="string">&#x27;306334678&#x27;</span>, <span class="string">&#x27;lqz12345&#x27;</span>, <span class="string">&#x27;903641&#x27;</span>)	<span class="comment">#用户中心&gt;&gt;软件ID 生成一个替换 96001</span></span><br><span class="line">    im = <span class="built_in">open</span>(<span class="string">&#x27;code.png&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>).read()											</span><br><span class="line">    <span class="comment"># print(chaojiying.PostPic(im, 9004))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## 返回结果如果有多个 260,133|123，233,处理这种格式[[260,133],[123,233]]</span></span><br><span class="line">    res=chaojiying.PostPic(im, <span class="number">9004</span>)</span><br><span class="line">    <span class="built_in">print</span>(res)</span><br><span class="line">    result=res[<span class="string">&#x27;pic_str&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    all_list = []</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;|&#x27;</span> <span class="keyword">in</span> result:</span><br><span class="line">        list_1 = result.split(<span class="string">&#x27;|&#x27;</span>)</span><br><span class="line">        count_1 = <span class="built_in">len</span>(list_1)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(count_1):</span><br><span class="line">            xy_list = []</span><br><span class="line">            x = <span class="built_in">int</span>(list_1[i].split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">            y = <span class="built_in">int</span>(list_1[i].split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">1</span>])</span><br><span class="line">            xy_list.append(x)</span><br><span class="line">            xy_list.append(y)</span><br><span class="line">            all_list.append(xy_list)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = <span class="built_in">int</span>(result.split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">        y = <span class="built_in">int</span>(result.split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">1</span>])</span><br><span class="line">        xy_list = []</span><br><span class="line">        xy_list.append(x)</span><br><span class="line">        xy_list.append(y)</span><br><span class="line">        all_list.append(xy_list)</span><br><span class="line">    <span class="built_in">print</span>(all_list)</span><br><span class="line">    <span class="comment"># 用动作链，点击图片</span></span><br><span class="line">    <span class="comment"># [[260,133],[123,233]]</span></span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> all_list:</span><br><span class="line">        x = a[<span class="number">0</span>]</span><br><span class="line">        y = a[<span class="number">1</span>]</span><br><span class="line">        ActionChains(bro).move_to_element_with_offset(img_t, x, y).click().perform()</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    username=bro.find_element_by_id(<span class="string">&#x27;J-userName&#x27;</span>)</span><br><span class="line">    username.send_keys(<span class="string">&#x27;306334678&#x27;</span>)</span><br><span class="line">    password=bro.find_element_by_id(<span class="string">&#x27;J-password&#x27;</span>)</span><br><span class="line">    password.send_keys(<span class="string">&#x27;lqz12345&#x27;</span>)</span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">    submit_login=bro.find_element_by_id(<span class="string">&#x27;J-login&#x27;</span>)</span><br><span class="line">    submit_login.click()</span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(bro.get_cookies())</span><br><span class="line">    time.sleep(<span class="number">10</span>)</span><br><span class="line">    bro.get(<span class="string">&#x27;https://www.12306.cn/index/&#x27;</span>)</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    bro.close()</span><br></pre></td></tr></table></figure>

<h2 id="自动给抽屉点赞"><a href="#自动给抽屉点赞" class="headerlink" title="自动给抽屉点赞"></a>自动给抽屉点赞</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">bro=webdriver.Chrome(executable_path=<span class="string">&#x27;./chromedriver.exe&#x27;</span>)</span><br><span class="line">bro.implicitly_wait(<span class="number">10</span>)</span><br><span class="line">bro.get(<span class="string">&#x27;https://dig.chouti.com/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">login_b=bro.find_element_by_id(<span class="string">&#x27;login_btn&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(login_b)</span><br><span class="line">login_b.click()</span><br><span class="line"></span><br><span class="line">username=bro.find_element_by_name(<span class="string">&#x27;phone&#x27;</span>)</span><br><span class="line">username.send_keys(<span class="string">&#x27;18953675221&#x27;</span>)</span><br><span class="line">password=bro.find_element_by_name(<span class="string">&#x27;password&#x27;</span>)</span><br><span class="line">password.send_keys(<span class="string">&#x27;lqz123&#x27;</span>)</span><br><span class="line"></span><br><span class="line">button=bro.find_element_by_css_selector(<span class="string">&#x27;button.login-btn&#x27;</span>)</span><br><span class="line">button.click()</span><br><span class="line"><span class="comment"># 可能有验证码，手动操作一下。</span></span><br><span class="line"><span class="comment"># 程序休眠时，浏览器会给出验证码，我们手动操作一下，输完验证码浏览器就会朝服务器发送请求完成登陆。</span></span><br><span class="line">time.sleep(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等程序结束休眠时，浏览器已经发送完请求完成登陆了，这时就可以拿到cookies了</span></span><br><span class="line">my_cookie=bro.get_cookies()  <span class="comment"># 列表</span></span><br><span class="line"><span class="built_in">print</span>(my_cookie)</span><br><span class="line">bro.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个cookie不是一个字典，不能直接给requests使用，需要转一下</span></span><br><span class="line">cookie=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> my_cookie:</span><br><span class="line">    cookie[item[<span class="string">&#x27;name&#x27;</span>]]=item[<span class="string">&#x27;value&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;https://dig.chouti.com/&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># ret = requests.get(&#x27;https://dig.chouti.com/&#x27;,headers=headers)</span></span><br><span class="line"><span class="comment"># print(ret.text)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入首页无须cookies，直接进入即可</span></span><br><span class="line">ret=requests.get(<span class="string">&#x27;https://dig.chouti.com/top/24hr?_=1596677637670&#x27;</span>,headers=headers)</span><br><span class="line"><span class="built_in">print</span>(ret.json())</span><br><span class="line">ll=[]</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> ret.json()[<span class="string">&#x27;data&#x27;</span>]:</span><br><span class="line">    ll.append(item[<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ll)</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> ll:</span><br><span class="line">    <span class="comment"># 点赞需要登陆才能操作，这时发post请求就需要 cookies了</span></span><br><span class="line">    ret=requests.post(<span class="string">&#x27; https://dig.chouti.com/link/vote&#x27;</span>,headers=headers,cookies=cookie,data=&#123;<span class="string">&#x27;linkId&#x27;</span>:<span class="built_in">id</span>&#125;)</span><br><span class="line">    <span class="built_in">print</span>(ret.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动评论</span></span><br><span class="line"><span class="string">&#x27;https://dig.chouti.com/comments/create&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">content: 说的号</span></span><br><span class="line"><span class="string">linkId: 29829529</span></span><br><span class="line"><span class="string">parentId: 0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="cookie池讲解-类似代理池"><a href="#cookie池讲解-类似代理池" class="headerlink" title="cookie池讲解(类似代理池)"></a>cookie池讲解(类似代理池)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如何搭建cookie池</span></span><br><span class="line"><span class="comment"># selenium写一套（一堆小号），跑起脚本，自动登录，手动参与</span></span><br><span class="line"><span class="comment"># 拿到cookie，放到redis中</span></span><br><span class="line"><span class="comment"># django搭建一个服务：127.0.0.0/get,随机返回一个cookie</span></span><br><span class="line"><span class="comment"># request发送请求爬数据（带着cookie池里的cookie），检测到cookie失效就删除cookie</span></span><br></pre></td></tr></table></figure>

<h2 id="抓包工具介绍"><a href="#抓包工具介绍" class="headerlink" title="抓包工具介绍"></a>抓包工具介绍</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">如何获得网页的信息</span><br><span class="line"><span class="comment"># 1 浏览器调试模式</span></span><br><span class="line"><span class="comment"># 2 抓包工具：fiddler，charles(自己研究一下)。。原理就是你的数据出去和进来都会经过这个抓包工具，它会给你解析出截获的数据</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h2><h3 id="Scrapy原理"><a href="#Scrapy原理" class="headerlink" title="Scrapy原理"></a>Scrapy原理</h3><p><img onerror="imgOnError(this);" data-fancybox="gallery" src="C:\Users\86134\AppData\Roaming\Typora\typora-user-images\image-20230116152046257.png" alt="image-20230116152046257" data-caption="image-20230116152046257" loading="lazy"></p>
<ol>
<li>SPIDERS对应爬虫文件中的parser函数，这个函数yield出url后，url会被封装成REQUESTS对象传到ENGINE</li>
<li>ENGINE把1中传过来的requests对象传到schedule中，schedule会把requests放到队列中，安排顺序</li>
<li>schedule调度队列中的requests对象，有序日到ENGINE</li>
<li>ENGINE把3中传来的requests通过download中间件传送到download中，由download负责发出请求并接收响应</li>
<li>download接收到响应并把响应封装成response对象通过download中间件传送到ENFGINE</li>
<li>ENGINE把5中传来的response对象通过spider中间件送到SPIDERS(即parser)中进行解析</li>
<li>SPIDERS把解析的结果通过SPIDER中间件送到ENGINE，可能是items，也可能是url(封装后成requests)</li>
<li>ENGINE判断7中传来的对象是什么，如果是items对象就送到item pipelines中进行存储；如果是requests对象就送到schedule里继续重复3-8</li>
</ol>
<h3 id="scrapy-介绍"><a href="#scrapy-介绍" class="headerlink" title="scrapy 介绍"></a>scrapy 介绍</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1  通用的网络爬虫框架,爬虫界的django</span></span><br><span class="line"><span class="comment">#2 scrapy执行流程</span></span><br><span class="line">	<span class="number">5</span>大组件</span><br><span class="line">    	-引擎(EGINE)：大总管，负责控制数据的流向</span><br><span class="line">        -调度器(SCHEDULER)：由它来决定下一个要抓取的网址是什么，去重</span><br><span class="line">        -下载器(DOWLOADER)：用于下载网页内容, 并将网页内</span><br><span class="line">        容返回给EGINE，下载器是建立在twisted这个高效的异步模型上的</span><br><span class="line">        -爬虫(SPIDERS):开发人员自定义的类，用来解析responses，并且提取items，或者发送新的请求request</span><br><span class="line">        -项目管道(ITEM PIPLINES):在items被提取后负责处理它们，主要包括清理、验证、持久化（比如存到数据库）等操作</span><br><span class="line">	<span class="number">2</span>大中间件</span><br><span class="line">    	-爬虫中间件：位于EGINE和SPIDERS之间，主要工作是处理SPIDERS的输入和输出（用的很少）</span><br><span class="line">        -下载中间件：引擎和下载器之间，加代理，加头，集成selenium        </span><br><span class="line">        </span><br><span class="line"><span class="comment"># 3 开发者只需要在固定的位置写固定的代码即可（写的最多的spider）</span></span><br></pre></td></tr></table></figure>

<h3 id="scrapy安装（windows，mac，linux）"><a href="#scrapy安装（windows，mac，linux）" class="headerlink" title="scrapy安装（windows，mac，linux）"></a>scrapy安装（windows，mac，linux）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1 pip3 install scrapy（mac，linux）</span></span><br><span class="line"><span class="comment">#2 windows上（直接pip install scrapy 80%能成功，成功不了才按下面的走）</span></span><br><span class="line">	<span class="number">1</span>、pip3 install wheel <span class="comment">#安装后，便支持通过wheel文件安装软件，wheel文件官网：https://www.lfd.uci.edu/~gohlke/pythonlibs</span></span><br><span class="line">    <span class="number">3</span>、pip3 install lxml</span><br><span class="line">    <span class="number">4</span>、pip3 install pyopenssl</span><br><span class="line">    <span class="number">5</span>、下载并安装pywin32：https://sourceforge.net/projects/pywin32/files/pywin32/</span><br><span class="line">    <span class="number">6</span>、下载twisted的wheel文件：http://www.lfd.uci.edu/~gohlke/pythonlibs/<span class="comment">#twisted</span></span><br><span class="line">    <span class="number">7</span>、执行pip3 install 下载目录\Twisted-<span class="number">17.9</span><span class="number">.0</span>-cp36-cp36m-win_amd64.whl</span><br><span class="line">    <span class="number">8</span>、pip3 install scrapy</span><br><span class="line"><span class="comment"># 3 安装成功就有scrapy命令</span></span><br><span class="line">	-D:\Python36\Scripts\scrapy.exe  用于创建项目</span><br></pre></td></tr></table></figure>

<h3 id="scrapy-创建项目，创建爬虫，运行爬虫"><a href="#scrapy-创建项目，创建爬虫，运行爬虫" class="headerlink" title="scrapy 创建项目，创建爬虫，运行爬虫"></a>scrapy 创建项目，创建爬虫，运行爬虫</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> scrapy startproject 项目名</span><br><span class="line">	-scrapy startproject firstscrapy</span><br><span class="line"><span class="number">2</span> 创建爬虫 </span><br><span class="line">	-scrapy genspider 爬虫名 爬虫地址</span><br><span class="line">    -scrapy genspider chouti dig.chouti.com</span><br><span class="line">    -一执行就会在spider文件夹下创建出一个py文件，名字叫chouti</span><br><span class="line"><span class="number">3</span> 运行爬虫</span><br><span class="line">	-scrapy crawl chouti   <span class="comment"># 带运行日志</span></span><br><span class="line">    -scrapy crawl chouti --nolog  <span class="comment"># 不带日志</span></span><br><span class="line"><span class="number">4</span> 支持右键执行爬虫</span><br><span class="line">	-在项目路径下新建一个main.py</span><br><span class="line">    <span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line">	execute([<span class="string">&#x27;scrapy&#x27;</span>,<span class="string">&#x27;crawl&#x27;</span>,<span class="string">&#x27;chouti&#x27;</span>,<span class="string">&#x27;--nolog&#x27;</span>])</span><br></pre></td></tr></table></figure>

<h3 id="目录介绍"><a href="#目录介绍" class="headerlink" title="目录介绍"></a>目录介绍</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 目录介绍</span></span><br><span class="line">firstscrapy  <span class="comment"># 项目名字</span></span><br><span class="line">    firstscrapy <span class="comment"># 包</span></span><br><span class="line">        -spiders <span class="comment"># 所有的爬虫文件放在里面。下面所提到的爬虫指的就是spiders文件夹里的py文件</span></span><br><span class="line">            -baidu.py <span class="comment"># 一个个的爬虫（以后基本上都在这写东西）</span></span><br><span class="line">            -chouti.py <span class="comment"># 函数中的spider参数就是这些文件，其中spider.name就是文件名，可以通过对spider.name的判断实现对不同的爬虫进行不同的处理</span></span><br><span class="line">        -middlewares.py <span class="comment"># 中间件（爬虫，下载中间件都写在这）</span></span><br><span class="line">        -pipelines.py   <span class="comment"># 持久化相关写在这（items.py中类的对象）</span></span><br><span class="line">        -main.py        <span class="comment"># 自己加的，右键执行爬虫</span></span><br><span class="line">        -items.py       <span class="comment"># 一个一个的类，</span></span><br><span class="line">        -settings.py    <span class="comment"># 配置文件</span></span><br><span class="line">    scrapy.cfg          <span class="comment"># 上线相关</span></span><br></pre></td></tr></table></figure>

<h3 id="settings介绍"><a href="#settings介绍" class="headerlink" title="settings介绍"></a>settings介绍</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> 默认情况，scrapy会去遵循爬虫协议</span><br><span class="line"><span class="number">2</span> 修改配置文件参数，强行爬取，不遵循协议</span><br><span class="line">	-ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"><span class="number">3</span> USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27;</span>  <span class="comment">#换请求头，否则默认的请求头是&#x27;scrapy&#x27;太爬虫了</span></span><br><span class="line"><span class="number">4</span> LOG_LEVEL=<span class="string">&#x27;ERROR&#x27;</span> <span class="comment"># 更改日志级别，只有error信息会被打印出来</span></span><br><span class="line"><span class="number">5</span> pipelines.py写的类要在settings中注册才有效</span><br><span class="line">	ITEM_PIPELINES = &#123;</span><br><span class="line">   		<span class="string">&#x27;firstscrapy.pipelines.ChoutiFilePipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure>

<h3 id="scrapy的数据解析（重点）"><a href="#scrapy的数据解析（重点）" class="headerlink" title="scrapy的数据解析（重点）"></a>scrapy的数据解析（重点）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scrapy自带的数据解析只有xpath和css</span><br><span class="line"><span class="comment">#xpath：</span></span><br><span class="line">    -response.xpath(<span class="string">&#x27;//a[contains(@class,&quot;link-title&quot;)]/text()&#x27;</span>).extract()  <span class="comment"># 取文本</span></span><br><span class="line">    -response.xpath(<span class="string">&#x27;//a[contains(@class,&quot;link-title&quot;)]/@href&#x27;</span>).extract()  <span class="comment">#取属性</span></span><br><span class="line"><span class="comment">#css</span></span><br><span class="line">	-response.css(<span class="string">&#x27;.link-title&#x27;</span>).extract()  <span class="comment"># 取标签</span></span><br><span class="line">    -response.css(<span class="string">&#x27;.link-title::text&#x27;</span>).extract()  <span class="comment"># 取文本</span></span><br><span class="line">    -response.css(<span class="string">&#x27;.link-title::attr(href)&#x27;</span>).extract_first()  <span class="comment"># 取属性</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="scrapy的持久化存储（重点）"><a href="#scrapy的持久化存储（重点）" class="headerlink" title="scrapy的持久化存储（重点）"></a>scrapy的持久化存储（重点）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1 方案一：parser函数必须返回列表套字典的形式（了解）</span></span><br><span class="line">	scrapy crawl chouti -o chouti.csv</span><br><span class="line"><span class="comment">#2 方案二：高级，pipline item存储（mysql，redis，文件）一般用这种</span></span><br><span class="line">	-在Items.py中写一个类</span><br><span class="line">    -在spider中导入，实例化，把数据放进去</span><br><span class="line">    	    item[<span class="string">&#x27;title&#x27;</span>]=title</span><br><span class="line">            item[<span class="string">&#x27;url&#x27;</span>]=url</span><br><span class="line">            item[<span class="string">&#x27;photo_url&#x27;</span>]=photo_url</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line">            </span><br><span class="line">    -在setting中配置（数字越小，级别越高）</span><br><span class="line">    	ITEM_PIPELINES = &#123;</span><br><span class="line">   		<span class="string">&#x27;firstscrapy.pipelines.ChoutiFilePipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">		&#125;</span><br><span class="line">    -在pipelines.py中写ChoutiFilePipeline</span><br><span class="line">    	-open_spider（开始的时候）</span><br><span class="line">        -close_spider（结束的时候）</span><br><span class="line">        -process_item（在这持久化）</span><br></pre></td></tr></table></figure>

<h3 id="自动给抽屉点赞-1"><a href="#自动给抽屉点赞-1" class="headerlink" title="自动给抽屉点赞"></a>自动给抽屉点赞</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">bro=webdriver.Chrome(executable_path=<span class="string">&#x27;./chromedriver.exe&#x27;</span>)</span><br><span class="line">bro.implicitly_wait(<span class="number">10</span>)</span><br><span class="line">bro.get(<span class="string">&#x27;https://dig.chouti.com/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">login_b=bro.find_element_by_id(<span class="string">&#x27;login_btn&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(login_b)</span><br><span class="line">login_b.click()</span><br><span class="line"></span><br><span class="line">username=bro.find_element_by_name(<span class="string">&#x27;phone&#x27;</span>)</span><br><span class="line">username.send_keys(<span class="string">&#x27;18953675221&#x27;</span>)</span><br><span class="line">password=bro.find_element_by_name(<span class="string">&#x27;password&#x27;</span>)</span><br><span class="line">password.send_keys(<span class="string">&#x27;lqz123&#x27;</span>)</span><br><span class="line"></span><br><span class="line">button=bro.find_element_by_css_selector(<span class="string">&#x27;button.login-btn&#x27;</span>)</span><br><span class="line">button.click()</span><br><span class="line"><span class="comment"># 可能有验证码，手动操作一下。</span></span><br><span class="line"><span class="comment"># 程序休眠时，浏览器会给出验证码，我们手动操作一下，输完验证码浏览器就会朝服务器发送请求完成登陆。</span></span><br><span class="line">time.sleep(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等程序结束休眠时，浏览器已经发送完请求完成登陆了，这时就可以拿到cookies了</span></span><br><span class="line">my_cookie=bro.get_cookies()  <span class="comment"># 列表</span></span><br><span class="line"><span class="built_in">print</span>(my_cookie)</span><br><span class="line">bro.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个cookie不是一个字典，不能直接给requests使用，需要转一下</span></span><br><span class="line">cookie=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> my_cookie:</span><br><span class="line">    cookie[item[<span class="string">&#x27;name&#x27;</span>]]=item[<span class="string">&#x27;value&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;https://dig.chouti.com/&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># ret = requests.get(&#x27;https://dig.chouti.com/&#x27;,headers=headers)</span></span><br><span class="line"><span class="comment"># print(ret.text)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入首页无须cookies，直接进入即可</span></span><br><span class="line">ret=requests.get(<span class="string">&#x27;https://dig.chouti.com/top/24hr?_=1596677637670&#x27;</span>,headers=headers)</span><br><span class="line"><span class="built_in">print</span>(ret.json())</span><br><span class="line">ll=[]</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> ret.json()[<span class="string">&#x27;data&#x27;</span>]:</span><br><span class="line">    ll.append(item[<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ll)</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> ll:</span><br><span class="line">    <span class="comment"># 点赞需要登陆才能操作，这时发post请求就需要 cookies了</span></span><br><span class="line">    ret=requests.post(<span class="string">&#x27; https://dig.chouti.com/link/vote&#x27;</span>,headers=headers,cookies=cookie,data=&#123;<span class="string">&#x27;linkId&#x27;</span>:<span class="built_in">id</span>&#125;)</span><br><span class="line">    <span class="built_in">print</span>(ret.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动评论</span></span><br><span class="line"><span class="string">&#x27;https://dig.chouti.com/comments/create&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">content: 说的号</span></span><br><span class="line"><span class="string">linkId: 29829529</span></span><br><span class="line"><span class="string">parentId: 0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="scrapy的请求传参，meta"><a href="#scrapy的请求传参，meta" class="headerlink" title="scrapy的请求传参，meta"></a>scrapy的请求传参，meta</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">本质就是在download中间件中，程序把 Request中的meta参数复制了一份给response中的meta了</span><br><span class="line"><span class="comment"># 把要传递的数据放到meta中</span></span><br><span class="line"><span class="keyword">yield</span> Request(urlmeta=&#123;<span class="string">&#x27;item&#x27;</span>:item&#125;)</span><br><span class="line"><span class="comment"># 在response对象中取出来</span></span><br><span class="line">item=response.meta.get(<span class="string">&#x27;item&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="scrapy的中间件（download中间件）"><a href="#scrapy的中间件（download中间件）" class="headerlink" title="scrapy的中间件（download中间件）"></a>scrapy的中间件（download中间件）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 都写在middlewares.py</span></span><br><span class="line"><span class="comment"># 2 爬虫中间件(用得少)</span></span><br><span class="line"><span class="comment"># 3 下载中间件</span></span><br><span class="line"><span class="comment"># 4 要生效，一定要配置，配置文件</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载中间件</span></span><br><span class="line">-process_request：返回不同的对象，后续处理不同（可以返回none、request、response、exception这四种对象，scrapy对每一种对象的处理方式不同，可以通过源码了解）</span><br><span class="line">  		<span class="comment"># 1 更换请求头</span></span><br><span class="line">        <span class="comment"># from scrapy.http.headers import Headers</span></span><br><span class="line">        <span class="comment"># request.headers[&#x27;User-Agent&#x27;]=&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 加cookie ---cookie池</span></span><br><span class="line">        <span class="comment"># 假设你你已经搭建好cookie 池了，</span></span><br><span class="line">        <span class="comment"># print(&#x27;00000--&#x27;,request.cookies)</span></span><br><span class="line">        <span class="comment"># request.cookies=&#123;&#x27;username&#x27;:&#x27;asdfasdf&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3 加代理，在meta里面加</span></span><br><span class="line">        <span class="comment"># print(request.meta)</span></span><br><span class="line">        <span class="comment"># request.meta[&#x27;download_timeout&#x27;] = 20</span></span><br><span class="line">        <span class="comment"># request.meta[&quot;proxy&quot;] = &#x27;http://27.188.62.3:8060&#x27;</span></span><br><span class="line">-process_response：返回不同的对象，后续处理不同</span><br><span class="line">- process_exception <span class="comment"># 有异常就会走这个函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;xxxx&#x27;</span>)</span><br><span class="line">        <span class="comment"># 不允许直接改url， 只能重新构造Requests对象返回</span></span><br><span class="line">        <span class="comment"># request.url=&#x27;https://www.baidu.com&#x27;</span></span><br><span class="line">        <span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</span><br><span class="line">        request=Request(url=<span class="string">&#x27;https://www.baidu.com&#x27;</span>,callback=spider.parser) <span class="comment"># callback参数是异步回调参数，表示当结果返回时用哪个函数来处理返回的结果</span></span><br><span class="line">        <span class="keyword">return</span> request</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="selenium在scrapy中的使用流程"><a href="#selenium在scrapy中的使用流程" class="headerlink" title="selenium在scrapy中的使用流程"></a>selenium在scrapy中的使用流程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 要让当前所有爬虫用的selenium是同一个浏览器。如果在中间件中起浏览器的话就是一个爬虫对应一个浏览器了，那样资源不够的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 在爬虫中初始化webdriver对象。这里的爬虫指的是spider文件夹里的py文件</span></span><br><span class="line">    <span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">CnblogSpider</span>(scrapy.Spider):</span><br><span class="line">        name = <span class="string">&#x27;cnblog&#x27;</span></span><br><span class="line">        ...</span><br><span class="line"> 		bro=webdriver.Chrome(executable_path=<span class="string">&#x27;../chromedriver.exe&#x27;</span>)</span><br><span class="line"><span class="comment"># 2 在中间件中使用（process_request） 使用中间件时记得去settings里面注册</span></span><br><span class="line">    spider.bro.get(<span class="string">&#x27;https://dig.chouti.com/&#x27;</span>)   	response=HtmlResponse(url=<span class="string">&#x27;https://dig.chouti.com/&#x27;</span>,body=spider.bro.page_source.encode(<span class="string">&#x27;utf-8&#x27;</span>),request=request)</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line">	</span><br><span class="line"><span class="comment"># 3 在爬虫中关闭</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close</span>(<span class="params">self, reason</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;我结束了&quot;</span>)</span><br><span class="line">        self.bro.close()</span><br></pre></td></tr></table></figure>

<h3 id="分布式爬虫（scrapy-redis）"><a href="#分布式爬虫（scrapy-redis）" class="headerlink" title="分布式爬虫（scrapy-redis）"></a>分布式爬虫（scrapy-redis）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">分布式爬虫的原理是，在redis中开辟一个共享队列，把要爬取的所有url都放在里面，所有的机器(或者进程)都去共享队列里拿url爬并进行解析，再把解析的结果存到各自的数据库里。如果用的是同一个数据库的表，那么数据也会被拼在一起</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 pip3 install scrapy-redis</span></span><br><span class="line"><span class="comment"># 2 原来继承Spider的类，现在改为继承RedisSpider</span></span><br><span class="line"><span class="comment"># 3 不能写start_urls = [&#x27;https:/www.cnblogs.com/&#x27;]，需要改成写redis_key = &#x27;myspider:start_urls&#x27;</span></span><br><span class="line"><span class="comment"># 4 setting中配置：</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># redis的连接</span></span><br><span class="line">REDIS_HOST = <span class="string">&#x27;localhost&#x27;</span>                            <span class="comment"># 主机名</span></span><br><span class="line">REDIS_PORT = <span class="number">6379</span>                                   <span class="comment"># 端口</span></span><br><span class="line">	<span class="comment"># 使用scrapy-redis的去重</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">&quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span></span><br><span class="line"><span class="comment"># 使用scrapy-redis的Scheduler</span></span><br><span class="line"><span class="comment"># 分布式爬虫的配置</span></span><br><span class="line">SCHEDULER = <span class="string">&quot;scrapy_redis.scheduler.Scheduler&quot;</span></span><br><span class="line"><span class="comment"># 持久化的可以配置，也可以不配置。如果进行了配置，那么解析出的数据也会存一份在redis里</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;scrapy_redis.pipelines.RedisPipeline&#x27;</span>: <span class="number">299</span> <span class="comment">#数字表示优先级，越小优先级越高</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5现在要让爬虫运行起来，需要去redis中以myspider:start_urls为key，插入一个起始地址：</span></span><br><span class="line">lpush myspider:start_urls https://www.cnblogs.com/</span><br></pre></td></tr></table></figure>

<h2 id="破解知乎登陆-js逆向和解密"><a href="#破解知乎登陆-js逆向和解密" class="headerlink" title="破解知乎登陆(js逆向和解密)"></a>破解知乎登陆(js逆向和解密)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#js逆向和解密：有些网站在post数据的时候会对数据用js代码进行加密，这时如果我们相拥request发送提交请求时就要带上加密的数据才行。那我们首先要破解它的加密方式，再把加密方式应用于我们的数据上。破解加密方式就是从网站加密后的结果逆向猜测它加密的方式，可以通过前端控制台的debug调试去猜测。如果猜得出来那还行，如果猜不出来，就只能扣出网页加密模块的js代码再用python中的模块去直接执行js代码得到加密数据了</span></span><br><span class="line"></span><br><span class="line">知乎post数据的格式</span><br><span class="line">client_id=c3cef7c66a1843f8b3a9e6a1e3160e20&amp;</span><br><span class="line">grant_type=password&amp;</span><br><span class="line">timestamp=<span class="number">1596702006088</span>&amp;</span><br><span class="line">source=com.zhihu.web&amp;</span><br><span class="line">signature=eac4a6c461f9edf86ef33ef950c7b6aa426dbb39&amp;</span><br><span class="line">username=%2B86liuqingzheng&amp;</span><br><span class="line">password=<span class="number">1111111</span>&amp;</span><br><span class="line">captcha=&amp;</span><br><span class="line">lang=en&amp;</span><br><span class="line">utm_source=&amp;</span><br><span class="line">ref_source=other_https%3A%2F%2Fwww.zhihu.com%2Fsignin%3Fnext%3D%252<span class="string">F&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 破解知乎登陆</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">import requests    #请求解析库</span></span><br><span class="line"><span class="string">import base64							  #base64解密加密库，用于base64格式的图片的存取</span></span><br><span class="line"><span class="string">from PIL import Image	  			      #图片处理库</span></span><br><span class="line"><span class="string">import hmac								  #加密库</span></span><br><span class="line"><span class="string">from hashlib import sha1				  #加密库</span></span><br><span class="line"><span class="string">import time</span></span><br><span class="line"><span class="string">from urllib.parse import urlencode		  #url编码库</span></span><br><span class="line"><span class="string">import execjs							  #python调用node.js</span></span><br><span class="line"><span class="string">from http import cookiejar as cookielib</span></span><br><span class="line"><span class="string">class Spider():</span></span><br><span class="line"><span class="string">    def __init__(self):</span></span><br><span class="line"><span class="string">        self.session = requests.session()</span></span><br><span class="line"><span class="string">        self.session.cookies = cookielib.LWPCookieJar()    #使cookie可以调用save和load方法</span></span><br><span class="line"><span class="string">        self.login_page_url = &#x27;https://www.zhihu.com/signin?next=%2F&#x27;</span></span><br><span class="line"><span class="string">        self.login_api = &#x27;https://www.zhihu.com/api/v3/oauth/sign_in&#x27;</span></span><br><span class="line"><span class="string">        self.captcha_api = &#x27;https://www.zhihu.com/api/v3/oauth/captcha?lang=en&#x27;</span></span><br><span class="line"><span class="string">        self.headers = <span class="subst">&#123;</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;user-agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36 LBBROWSER&#x27;</span>,</span></span></span><br><span class="line"><span class="subst"><span class="string">        &#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        self.captcha =&#x27;&#x27;         #存验证码</span></span><br><span class="line"><span class="string">        self.signature = &#x27;&#x27;	   #存签名</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # 首次请求获取cookie</span></span><br><span class="line"><span class="string">    def get_base_cookie(self):</span></span><br><span class="line"><span class="string">        self.session.get(url=self.login_page_url, headers=self.headers)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def deal_captcha(self):</span></span><br><span class="line"><span class="string">        r = self.session.get(url=self.captcha_api, headers=self.headers)</span></span><br><span class="line"><span class="string">        r = r.json()</span></span><br><span class="line"><span class="string">        if r.get(&#x27;show_captcha&#x27;):</span></span><br><span class="line"><span class="string">            while True:</span></span><br><span class="line"><span class="string">                r = self.session.put(url=self.captcha_api, headers=self.headers)</span></span><br><span class="line"><span class="string">                img_base64 = r.json().get(&#x27;img_base64&#x27;)</span></span><br><span class="line"><span class="string">                with open(&#x27;captcha.png&#x27;, &#x27;wb&#x27;) as f:</span></span><br><span class="line"><span class="string">                    f.write(base64.b64decode(img_base64))</span></span><br><span class="line"><span class="string">                captcha_img = Image.open(&#x27;captcha.png&#x27;)</span></span><br><span class="line"><span class="string">                captcha_img.show()</span></span><br><span class="line"><span class="string">                self.captcha = input(&#x27;输入验证码:&#x27;)</span></span><br><span class="line"><span class="string">                r = self.session.post(url=self.captcha_api, data=<span class="subst">&#123;<span class="string">&#x27;input_text&#x27;</span>: self.captcha&#125;</span>,</span></span><br><span class="line"><span class="string">                                      headers=self.headers)</span></span><br><span class="line"><span class="string">                if r.json().get(&#x27;success&#x27;):</span></span><br><span class="line"><span class="string">                    break</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def get_signature(self):</span></span><br><span class="line"><span class="string">        # 生成加密签名</span></span><br><span class="line"><span class="string">        a = hmac.new(b&#x27;d1b964811afb40118a12068ff74a12f4&#x27;, digestmod=sha1)</span></span><br><span class="line"><span class="string">        a.update(b&#x27;password&#x27;)</span></span><br><span class="line"><span class="string">        a.update(b&#x27;c3cef7c66a1843f8b3a9e6a1e3160e20&#x27;)</span></span><br><span class="line"><span class="string">        a.update(b&#x27;com.zhihu.web&#x27;)</span></span><br><span class="line"><span class="string">        a.update(str(int(time.time() * 1000)).encode(&#x27;utf-8&#x27;))</span></span><br><span class="line"><span class="string">        self.signature = a.hexdigest()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def post_login_data(self):</span></span><br><span class="line"><span class="string">        data = <span class="subst">&#123;</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;client_id&#x27;</span>: <span class="string">&#x27;c3cef7c66a1843f8b3a9e6a1e3160e20&#x27;</span>,</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;grant_type&#x27;</span>: <span class="string">&#x27;password&#x27;</span>,</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;timestamp&#x27;</span>: <span class="built_in">str</span>(<span class="built_in">int</span>(time.time() * <span class="number">1000</span>)),</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;com.zhihu.web&#x27;</span>,</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;signature&#x27;</span>: self.signature,</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;username&#x27;</span>: <span class="string">&#x27;+8618953675221&#x27;</span>,</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;password&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;captcha&#x27;</span>: self.captcha,</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;lang&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;utm_source&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;ref_source&#x27;</span>: <span class="string">&#x27;other_https://www.zhihu.com/signin?next=%2F&#x27;</span>,</span></span></span><br><span class="line"><span class="subst"><span class="string">        &#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        headers = <span class="subst">&#123;</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;x-zse-83&#x27;</span>: <span class="string">&#x27;3_2.0&#x27;</span>,</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;content-type&#x27;</span>: <span class="string">&#x27;application/x-www-form-urlencoded&#x27;</span>,</span></span></span><br><span class="line"><span class="subst"><span class="string">            <span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36 LBBROWSER&#x27;</span>,</span></span></span><br><span class="line"><span class="subst"><span class="string">        &#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        data = urlencode(data)</span></span><br><span class="line"><span class="string">        with open(&#x27;zhih.js&#x27;, &#x27;rt&#x27;, encoding=&#x27;utf-8&#x27;) as f:</span></span><br><span class="line"><span class="string">            js = execjs.compile(f.read(), cwd=&#x27;node_modules&#x27;)</span></span><br><span class="line"><span class="string">        data = js.call(&#x27;b&#x27;, data)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        r = self.session.post(url=self.login_api, headers=headers, data=data)</span></span><br><span class="line"><span class="string">        print(r.text)</span></span><br><span class="line"><span class="string">        if r.status_code == 201:</span></span><br><span class="line"><span class="string">            self.session.cookies.save(&#x27;mycookie&#x27;)</span></span><br><span class="line"><span class="string">            print(&#x27;登录成功&#x27;)</span></span><br><span class="line"><span class="string">        else:</span></span><br><span class="line"><span class="string">            print(&#x27;登录失败&#x27;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def login(self):</span></span><br><span class="line"><span class="string">        self.get_base_cookie()</span></span><br><span class="line"><span class="string">        self.deal_captcha()</span></span><br><span class="line"><span class="string">        self.get_signature()</span></span><br><span class="line"><span class="string">        self.post_login_data()</span></span><br><span class="line"><span class="string">if __name__ == &#x27;__main__&#x27;:</span></span><br><span class="line"><span class="string">    zhihu_spider = Spider()</span></span><br><span class="line"><span class="string">    zhihu_spider.login()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>

<h2 id="爬虫的反扒措施"><a href="#爬虫的反扒措施" class="headerlink" title="爬虫的反扒措施"></a>爬虫的反扒措施</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> user-agent</span><br><span class="line"><span class="number">2</span> referer</span><br><span class="line"><span class="number">3</span> cookie（访问多了可能封ip，可以用cookie池；有些网站访问首页的cookie和用户的不一样，可以先访问一次拿到cookie）</span><br><span class="line"><span class="number">4</span> 频率限制（代理池；times.sleep延迟）</span><br><span class="line"><span class="number">5</span> js加密（扣出js代码，exjs模块执行js代码）</span><br><span class="line"><span class="number">6</span> css加密</span><br><span class="line"><span class="number">7</span> 验证码（打码平台），半手动输入验证码</span><br><span class="line"><span class="number">8</span> 图片懒加载。有些网站如果你不下滑，下面的图片就不加载，这时这些图片的src存在对应标签的lazy-img属性里，等到需要加载图片时再把lazy-img的值放到src里进行加载</span><br></pre></td></tr></table></figure>




    <p><img data-fancybox="gallery" src="/pictures/a2.png"  alt="/pictures/a2.png" data-caption="/pictures/a2.png" loading="lazy"></p>
  </article>

  
      
    <div class="nexmoe-post-copyright">
        <strong>Author：</strong>czw<br>
        <strong>Link：</strong><a href="http://example.com/2023/02/06/%E7%88%AC%E8%99%AB/" title="http:&#x2F;&#x2F;example.com&#x2F;2023&#x2F;02&#x2F;06&#x2F;%E7%88%AC%E8%99%AB&#x2F;" target="_blank" rel="noopener">http:&#x2F;&#x2F;example.com&#x2F;2023&#x2F;02&#x2F;06&#x2F;%E7%88%AC%E8%99%AB&#x2F;</a><br>
        
            <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可
        
    </div>


  
  
  <div class="nexmoe-post-meta nexmoe-rainbow">
   
    
        <a class="nexmoefont icon-tag-fill -none-link" href="/tags/code/" rel="tag">code</a> <a class="nexmoefont icon-tag-fill -none-link" href="/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/" rel="tag">知识分享</a>
    
</div>

  
      <div class="nexmoe-post-footer">
          
      </div>
  
</div>
            <div class="nexmoe-post-right">
              <div class="nexmoe-fixed">
                  <div class="nexmoe-tool"> 
                    
                      
                        
                          
                          
                              <button class="mdui-fab catalog" style="overflow:unset;">
                                  <i class="nexmoefont icon-i-catalog"></i>
                                  <div class="nexmoe-toc">
                                      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#BUG"><span class="toc-number">1.</span> <span class="toc-text">BUG</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tips"><span class="toc-number">2.</span> <span class="toc-text">Tips</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.</span> <span class="toc-text">爬虫介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#requests%E6%A8%A1%E5%9D%97%E4%BD%BF%E7%94%A8"><span class="toc-number">4.</span> <span class="toc-text">requests模块使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E6%9F%90%E7%BD%91%E7%AB%99"><span class="toc-number">5.</span> <span class="toc-text">模拟登陆某网站</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E6%A2%A8%E8%A7%86%E9%A2%91-re%E8%A7%A3%E6%9E%90%E6%96%87%E6%A1%A3"><span class="toc-number">6.</span> <span class="toc-text">爬取梨视频(re解析文档)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bs4%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">7.</span> <span class="toc-text">bs4的使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%90%86%E6%B1%A0"><span class="toc-number">8.</span> <span class="toc-text">代理池</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E7%A0%81%E7%A0%B4%E8%A7%A3%E4%B9%8B-%E6%89%93%E7%A0%81%E5%B9%B3%E5%8F%B0%E4%BB%8B%E7%BB%8D"><span class="toc-number">9.</span> <span class="toc-text">验证码破解之-打码平台介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#xpath%E9%80%89%E6%8B%A9%E5%99%A8%E4%BD%BF%E7%94%A8"><span class="toc-number">10.</span> <span class="toc-text">xpath选择器使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#selenium%E4%BD%BF%E7%94%A8"><span class="toc-number">11.</span> <span class="toc-text">selenium使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E6%8B%89%E5%8B%BE%E7%BD%91%E8%81%8C%E4%BD%8D%E4%BF%A1%E6%81%AF-%E4%B8%8D%E5%90%8C%E9%A1%B5%E9%9D%A2cookie%E4%B8%8D%E4%B8%80%E6%A0%B7%EF%BC%8C%E5%BE%97%E5%85%88%E6%8B%BF%E5%88%B0cookie"><span class="toc-number">12.</span> <span class="toc-text">爬拉勾网职位信息(不同页面cookie不一样，得先拿到cookie)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%9512306"><span class="toc-number">13.</span> <span class="toc-text">自动登录12306</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E7%BB%99%E6%8A%BD%E5%B1%89%E7%82%B9%E8%B5%9E"><span class="toc-number">14.</span> <span class="toc-text">自动给抽屉点赞</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cookie%E6%B1%A0%E8%AE%B2%E8%A7%A3-%E7%B1%BB%E4%BC%BC%E4%BB%A3%E7%90%86%E6%B1%A0"><span class="toc-number">15.</span> <span class="toc-text">cookie池讲解(类似代理池)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D"><span class="toc-number">16.</span> <span class="toc-text">抓包工具介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy"><span class="toc-number">17.</span> <span class="toc-text">Scrapy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Scrapy%E5%8E%9F%E7%90%86"><span class="toc-number">17.1.</span> <span class="toc-text">Scrapy原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy-%E4%BB%8B%E7%BB%8D"><span class="toc-number">17.2.</span> <span class="toc-text">scrapy 介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy%E5%AE%89%E8%A3%85%EF%BC%88windows%EF%BC%8Cmac%EF%BC%8Clinux%EF%BC%89"><span class="toc-number">17.3.</span> <span class="toc-text">scrapy安装（windows，mac，linux）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy-%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE%EF%BC%8C%E5%88%9B%E5%BB%BA%E7%88%AC%E8%99%AB%EF%BC%8C%E8%BF%90%E8%A1%8C%E7%88%AC%E8%99%AB"><span class="toc-number">17.4.</span> <span class="toc-text">scrapy 创建项目，创建爬虫，运行爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95%E4%BB%8B%E7%BB%8D"><span class="toc-number">17.5.</span> <span class="toc-text">目录介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#settings%E4%BB%8B%E7%BB%8D"><span class="toc-number">17.6.</span> <span class="toc-text">settings介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy%E7%9A%84%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%EF%BC%88%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-number">17.7.</span> <span class="toc-text">scrapy的数据解析（重点）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8%EF%BC%88%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-number">17.8.</span> <span class="toc-text">scrapy的持久化存储（重点）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E7%BB%99%E6%8A%BD%E5%B1%89%E7%82%B9%E8%B5%9E-1"><span class="toc-number">17.9.</span> <span class="toc-text">自动给抽屉点赞</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy%E7%9A%84%E8%AF%B7%E6%B1%82%E4%BC%A0%E5%8F%82%EF%BC%8Cmeta"><span class="toc-number">17.10.</span> <span class="toc-text">scrapy的请求传参，meta</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy%E7%9A%84%E4%B8%AD%E9%97%B4%E4%BB%B6%EF%BC%88download%E4%B8%AD%E9%97%B4%E4%BB%B6%EF%BC%89"><span class="toc-number">17.11.</span> <span class="toc-text">scrapy的中间件（download中间件）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#selenium%E5%9C%A8scrapy%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E6%B5%81%E7%A8%8B"><span class="toc-number">17.12.</span> <span class="toc-text">selenium在scrapy中的使用流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%EF%BC%88scrapy-redis%EF%BC%89"><span class="toc-number">17.13.</span> <span class="toc-text">分布式爬虫（scrapy-redis）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A0%B4%E8%A7%A3%E7%9F%A5%E4%B9%8E%E7%99%BB%E9%99%86-js%E9%80%86%E5%90%91%E5%92%8C%E8%A7%A3%E5%AF%86"><span class="toc-number">18.</span> <span class="toc-text">破解知乎登陆(js逆向和解密)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E7%9A%84%E5%8F%8D%E6%89%92%E6%8E%AA%E6%96%BD"><span class="toc-number">19.</span> <span class="toc-text">爬虫的反扒措施</span></a></li></ol>
                                  </div>
                              </button>
                          
                          
                      
                    
                      <a href="#nexmoe-content" class="toc-link" aria-label="Back To Top" title="top"><button class="mdui-fab mdui-ripple"><i class="nexmoefont icon-caret-top"></i></button></a>
                  </div>
              </div>
            </div>
        </div>
    </div>
    <div id="nexmoe-search-space">
	<div class="search-container">
		<div class="search-header">
			<div class="search-input-container">
				<input
					class="search-input"
					type="text"
					placeholder="Search"
					oninput="sinput();"
				/>
			</div>
			<a class="search-close" onclick="sclose();">×</a>
		</div>
		<div class="search-body"></div>
	</div>
</div>

    
<script src="/lib/mdui_043tiny/mdui.js"></script>
<script src="/lib/fancybox/fancybox.umd.js"></script>


 

<script async src="/js/app.js?v=1675675995171"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2058306854838448" crossorigin="anonymous"></script>

</body>

</html>
