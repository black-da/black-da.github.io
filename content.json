{"meta":{"title":"czw","subtitle":"subtitile of czw","description":"","author":"czw","url":"http://example.com","root":"/"},"pages":[{"title":"文章归档","date":"2023-02-04T00:47:46.289Z","updated":"2023-02-02T09:13:35.800Z","comments":true,"path":"archives.html","permalink":"http://example.com/archives.html","excerpt":"","text":""},{"title":"","date":"2023-02-03T03:23:49.649Z","updated":"2023-02-03T03:23:49.643Z","comments":true,"path":"about.html","permalink":"http://example.com/about.html","excerpt":"","text":"我是czw华电的计科的"},{"title":"","date":"2023-02-04T01:49:19.764Z","updated":"2023-02-04T01:49:19.764Z","comments":true,"path":"PY.html","permalink":"http://example.com/PY.html","excerpt":"","text":"四海之内皆兄弟哈哈哈"},{"title":"","date":"2023-02-04T01:52:25.025Z","updated":"2023-02-04T01:52:25.025Z","comments":true,"path":"donate.html","permalink":"http://example.com/donate.html","excerpt":"","text":""}],"posts":[{"title":"爬虫","slug":"爬虫","date":"2023-02-06T09:25:23.939Z","updated":"2023-02-06T09:23:06.676Z","comments":true,"path":"2023/02/06/爬虫/","link":"","permalink":"http://example.com/2023/02/06/%E7%88%AC%E8%99%AB/","excerpt":"","text":"BUG123456789101112ValueError: check_hostname requires server_hostname在用翻墙访问页面时遇到这个bug，原因是翻墙的话会用到代理，这时就需要指定proxies代理参数才可以。即proxies = &#123;&#x27;http&#x27;: &#x27;http://localhost:7890&#x27;, &#x27;https&#x27;: &#x27;http://localhost:7890&#x27;&#125;，端口号是启用代理的端口号然后用res = requests.get(url, proxies=proxies)就可以正常访问了在遇到其他需要访问网址的页面都要加上proxies参数，比如调用谷歌翻译API进行翻译时，实例化对象时用client = Translate(proxies=&#123;&#x27;https&#x27;: &#x27;http://localhost:10809&#x27;&#125;)如果不用翻墙的话，直接把代理关了问题也可以解决了上面的处理，在遇到其他去要使用代理去访问网站的问题，比如通过某个包访问谷歌翻译时要访问网址还是会报一样的错误，除非手动加代理。下面的方法更普适一些其实产生这个问题的根本原因是urllib3版本太高，新版的urllib3修改了一些东西，所以会报错。下面的解决办法是降低urlib3的版本。在命令行运行pip install urllib3==1.25.11如果失败就运行pip install urllib3==1.25.11 -i http://pypi.douban.com/simple --trusted-host pypi.douban.com Tips XHR可以看到ajax请求 。一般直接朝url发请求返回的页面里如果没有想要的数据，那么想要的数据就是用ajax朝后端请求的，就要通过查看XHR来确定自己想要的资料在那个请求里 pip install -r README.md 要是ip被禁了，可以开vpn换个代理。也可以手动进去几次，干扰机器识别 有些网站访问不同的页面cookies是有变化的，针对这种情况可以先用session登陆一次拿到cookie，之后再用拿到的cookie继续发请求 遇到module not found的问题，可能是模块的版本不匹配的原因，直接把对应的模块pip uninstall了，再pip install 一下就好了 &#96;&#96;&#96;python用Python将网页上抓取下来的base64字符串转换成图片的形式保存到本地base64_str &#x3D; ‘data:image&#x2F;jpg;base64,&#x2F;9j&#x2F;4AAQSkZJRgABAQAAAQABAAD&#x2F;2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8’res &#x3D; base64_str.split(“,”)[1]img_base64_decode &#x3D; base64.b64decode(res)with open(‘a.jpg’, ‘wb’) as img:img.write(img_base64_decode)把图片转为base64二进制格式存储：with open(‘0.jpg’, ‘rb’) as f:content &#x3D; f.read()print(base64.b64encode(content)) 123456789101112131415161718192021222324* ```python 将页面滚动到最后的js // 定义函数 (function page_scroll() &#123; var i = 1 var element = document.documentElement element.scrollTop = 0; // 不管他在哪里，都让他先回到最上面 // 设置定时器，时间即为滚动速度 function main() &#123; if (element.scrollTop + element.clientHeight == element.scrollHeight) &#123; clearInterval(interval) console.log(&#x27;已经到底部了&#x27;) &#125; else &#123; element.scrollTop += 300; console.log(i); i += 1; &#125; &#125; // 定义ID interval = setInterval(main, 300) &#125;) () # page_scroll() 爬虫介绍123456781 爬虫：网络蜘蛛2 爬虫本质：模拟浏览器发送请求（requests，selenium）-&gt;下载网页代码-&gt;只提取有用的数据（bs4，xpath，re）-&gt;存放于数据库或文件中（文件，excel，mysql，redis，mongodb）3 发送请求：请求地址（浏览器调试，抓包工具），请求头，请求体，请求方法4 拿到响应：拿到响应体（json格式，xml格式，html格式（bs4,xpath），加密的未知格式(需要解密)）5 入库：Mongodb（json格式数据）6 性能高一些（多线程，多进程，协程），只针对与python语言的cpython解释器（GIL：同一时刻只能由一个线程在执行） -io密集型：用线程 -计算密集型：用进程 requests模块使用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201 安装：pip3 install requests2 图片防盗链：referer #存的是你是从什么网页跳转到当前网页的，如果你之前的网页不是这个服务器的网页，那么就不会给你数据。referer可以在网页的headers里找到3 代码import requests# 1 发送get请求# res是python的对象，对象里有响应头，响应体。。。。# header = &#123;# &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27;,# &#x27;referer&#x27;: &#x27;https://www.mzitu.com/225078/2&#x27;# &#125;# # res = requests.get(&#x27;https://www.mzitu.com/&#x27;, headers=header)# # print(res.text) # 网页的文本数据# res1 = requests.get(&#x27;https://i3.mmzztt.com/2020/03/14a02.jpg&#x27;, headers=header)# # print(res1.content) # 二进制内容，通常用于音频或图片的下载## with open(&#x27;a.jpg&#x27;, &#x27;wb&#x27;)as f:# for line in res1.iter_content():# f.write(line)# 2 请求地址中携带数据(两种方式，推荐第二种),在发送http请求时，通常会对url进行编码，如果url中存在中文的话，可能发送过去就变成乱码了，所以需要注意编码第1种 手动编码解码# from urllib.parse import urlencode,unquote # url的编码和解码# print(unquote(&#x27;%E7%BE%8E%E5%A5%B3&#x27;)) # 打印&#x27;美女&#x27;# header = &#123;# &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27;,# &#125;# # res=requests.get(&#x27;https://www.baidu.com/s?wd=美女&#x27;,headers=header)# 这里的美女发送过去可能因为中文编码的问题变成乱码，所以需要特殊处理一下# res=requests.get(&#x27;https://www.baidu.com/s&#x27;,headers=header,params=&#123;&#x27;wd&#x27;:&#x27;美女&#x27;&#125;)# print(res.url)# 3 请求带cookie(两种方式)# 方式一，在header中放# header = &#123;# &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27;,# &#x27;cookie&#x27;:&#x27;key=asdfasdfasdfsdfsaasdf;key2=asdfasdf;key3=asdfasdf&#x27;# &#125;# res=requests.get(&#x27;http://127.0.0.1:8000/index/&#x27;,headers=header)# 方式二：# header = &#123;# &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27;,# &#125;# # cookies是一个字典或者CookieJar对象# res=requests.get(&#x27;http://127.0.0.1:8000/index/&#x27;,headers=header,cookies=&#123;&#x27;key&#x27;:&#x27;asdfasdf&#x27;&#125;)# 4 发送post请求，携带数据（urlencoded和json）# res=requests.post(&#x27;http://127.0.0.1:8000/index/&#x27;,data=&#123;&#x27;name&#x27;:&#x27;lqz&#x27;&#125;)# res=requests.post(&#x27;http://127.0.0.1:8000/index/&#x27;,json=&#123;&#x27;age&#x27;:1,&#125;,)# 5 自动携带cookie# session=requests.session()# res=session.post(&#x27;http://127.0.0.1:8000/index/&#x27;) # 假设这个请求登录了# res1=session.get(&#x27;http://127.0.0.1:8000/order/&#x27;) # 现在不需要手动带cookie，session会帮咱处理# 6 response对象，就是前面的res# respone=requests.post(&#x27;http://127.0.0.1:8000/index/&#x27;,data=&#123;&#x27;name&#x27;:&#x27;lqz&#x27;&#125;)# # print(respone.text) # 响应的文本# print(respone.content) # 响应体的二进制### print(respone.status_code) # 响应状态码# print(respone.headers) # 响应头# print(respone.cookies) # cookie# print(respone.cookies.get_dict()) # 把cookie转成字典# print(respone.cookies.items()) # key和value[(key, value), ]# print(respone.url) # 请求的url# print(respone.history) #[]放重定向之前的地址# print(respone.encoding) # 响应的编码方式# respone.iter_content() # 图片，视频，大文件，一点一点循环取出来# for line in respone.iter_content():# f.write(line)# 7 编码问题# res=requests.get(&#x27;http://www.autohome.com/news&#x27;)# # 一旦打印出来出现乱码问题，那就是编码出错了# # 方式一# # res.encoding=&#x27;gb2312&#x27;# # 方式二# res.encoding=res.apparent_encoding，会帮你自动获取页面的编码方式# print(res.text)# 8 解析json# import json# respone=requests.post(&#x27;http://127.0.0.1:8000/index/&#x27;,data=&#123;&#x27;name&#x27;:&#x27;lqz&#x27;&#125;)# # print(json.loads(respone.text))# print(respone.json()) # 相当于上面那句话# 9 高级用法：使用代理。就是爬虫先向代理的ip发请求，再利用代理的ip去向目标网址发请求，这样目标网址封ip封的也是代理ip，而不是我的ip# respone=requests.get(&#x27;http://127.0.0.1:8000/index/&#x27;,proxies=&#123;&#x27;http&#x27;:&#x27;代理的地址和端口号&#x27;,&#125;)代理可以去百度# 代理，免费代理，收费代理花钱买# 代理池：列表放了一堆代理ip，每次随机取一个，再发请求就不会封ip了# 高匿和透明代理？如果使用高匿代理，后端无论如何拿不到你的ip，使用透明，后端能够拿到你的ip# 后端如何拿到透明代理的ip， 后端：X-Forwarded-For# respone=requests.get(&#x27;https://www.baidu.com/&#x27;,proxies=&#123;&#x27;http&#x27;:&#x27;27.46.20.226:8888&#x27;,&#125;)# print(respone.text)# 10 超时设置# import requests# respone=requests.get(&#x27;https://www.baidu.com&#x27;,# timeout=0.0001)# 13 异常处理# try:# r=requests.get(&#x27;http://www.baidu.com&#x27;,timeout=0.00001)# except Exception as e:# print(e)# 14 上传文件# res=requests.post(&#x27;http://127.0.0.1:8000/index/&#x27;,files=&#123;&#x27;myfile&#x27;:open(&#x27;a.jpg&#x27;,&#x27;rb&#x27;)&#125;) 模拟登陆某网站123456789101112131415161718192021#http://www.aa7a.cn/import requestssession=requests.session()data = &#123; &#x27;username&#x27;: &#x27;616564099@qq.com&#x27;, &#x27;password&#x27;: &#x27;lqz123&#x27;, &#x27;captcha&#x27;: &#x27;zdu4&#x27;, &#x27;remember&#x27;: 1, &#x27;ref&#x27;: &#x27;http://www.aa7a.cn/user.php?act=logout&#x27;, &#x27;act&#x27;: &#x27;act_login&#x27;,&#125;rest = session.post(&#x27;http://www.aa7a.cn/user.php&#x27;,data=data)print(rest.text)# 拿到cookiecookie=rest.cookiesprint(cookie)# 携带着cookies，表示登录了，页面中会有我们的用户信息616564099@qq.comrest1=session.get(&#x27;http://www.aa7a.cn/index.php&#x27;)# rest1=requests.get(&#x27;http://www.aa7a.cn/index.php&#x27;)print(&#x27;616564099@qq.com&#x27; in rest1.text) 爬取梨视频(re解析文档)123456789101112131415161718192021222324252627282930这网站改了，再网页里已经找不到mp4的信息了#https://www.pearvideo.com/# 爬取梨视频import requestsimport reres=requests.get(&#x27;https://www.pearvideo.com/category_loading.jsp?reqType=5&amp;categoryId=1&amp;start=0&#x27;)# print(res.text)re_video=&#x27;&lt;a href=&quot;(.*?)&quot; class=&quot;vervideo-lilink actplay&quot;&gt;&#x27;video_urls=re.findall(re_video,res.text)# https://www.pearvideo.com/# print(video_urls)for video in video_urls: url=&#x27;https://www.pearvideo.com/&#x27;+video print(url) # 向视频详情发送get请求 res_video=requests.get(url) # print(res_video.text) # break re_video_mp4=&#x27;hdUrl=&quot;&quot;,sdUrl=&quot;&quot;,ldUrl=&quot;&quot;,srcUrl=&quot;(.*?)&quot;,vdoUrl=srcUrl,skinRes&#x27; video_url=re.findall(re_video_mp4,res_video.text)[0] print(video_url) video_name=video_url.rsplit(&#x27;/&#x27;,1)[-1] print(video_name) res_video_content=requests.get(video_url) with open(video_name,&#x27;wb&#x27;) as f: for line in res_video_content.iter_content(): f.write(line) bs4的使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546本质上是一个文本解析器，可以解析html和xml格式的文档使用前导入：from bs4 import BeautifulSoupsoup = BeautifulSoup(document, parser) # 第一个参数是要解析的文档，第二个是解析器# 解析器一般用&#x27;html.parser&#x27;(Python内置的，直接用);# 或&#x27;lxml&#x27;,需要安装：pip install lxml# 搜索文档树# find() # 只返回找到的第一个# find_all() # 找到的所有# 五种过滤器: 字符串、正则表达式、列表、True、方法# 字符串过滤，通过字符串过滤内容# a=soup.find(name=&#x27;a&#x27;) 标签名# res=soup.find(id=&#x27;my_p&#x27;) id# res=soup.find(class_=&#x27;story&#x27;) class，避免关键字冲突，故用class_# res=soup.find(href=&#x27;http://example.com/elsie&#x27;) href# res=soup.find(id=&#x27;my_p&#x27;， name=&#x27;p&#x27;) id and name，是and的关系# res=soup.find(attrs=&#123;&#x27;id&#x27;:&#x27;my_p&#x27;&#125;) 用字典的方式也可# res=soup.find(attrs=&#123;&#x27;class&#x27;:&#x27;story&#x27;&#125;)# 正则表达式# import re# # re_b=re.compile(&#x27;^b&#x27;)# res=soup.find(name=re_b)# # res=soup.find_all(name=re_b)# res=soup.find_all(id=re.compile(&#x27;^l&#x27;))# 列表# res=soup.find_all(name=[&#x27;body&#x27;,&#x27;b&#x27;])# res=soup.find_all(class_=[&#x27;sister&#x27;,&#x27;title&#x27;])# True和false# res=soup.find_all(name=True)# res=soup.find_all(id=True)# res=soup.find_all(id=False)# res=soup.find_all(href=True)# css选择# ret=soup.select(&#x27;#my_p&#x27;) 返回的是个列表# https://www.w3school.com.cn/cssref/css_selectors.asp# ret=soup.select(&#x27;body p&#x27;) # 子子孙孙# ret=soup.select(&#x27;body&gt;p&#x27;) # 直接子节点（儿子）# ret=soup.select(&#x27;body&gt;p&#x27;)[0].text # 直接子节点（儿子）# # ret=soup.select(&#x27;body&gt;p&#x27;)[0].a.find() 可以混搭使用的，都是Tag对象 代理池1234567891011121314就是写一个爬虫去全网爬代理ip地址，然后保存在数据库再开一个web服务，每次有请求就随机从数据库取一个ip地址返回# github，下载免费代理池开源代码（建议读一下别人的代码）# git clone git@github.com:jhao104/proxy_pool.git# pycharm打开，修改配置文件（reids地址修改）# 启动爬虫：python proxyPool.py schedule# 启动服务：python3 proxyPool.py server# 随机获取一个代理requests.get(&quot;http://127.0.0.1:5010/get/&quot;).json()#删除一个代理requests.get(&quot;http://127.0.0.1:5010/delete/?proxy=&#123;&#125;&quot;.format(proxy)) 验证码破解之-打码平台介绍123456# 1 验证码破解 图像处理# 2 专业打码平台，破解验证码（收费）# 申请超级鹰，注册# 登录，下载sdk（代码如下），填入用户名密码，软件id#!/usr/bin/env python# coding:utf-8 xpath选择器使用12345678910111213141516171819202122232425262728293031323334353637383940# 查找标签的三种方式，bs4的find， css选择器，xpath查找（后2种时许多语言通用的）# xpath: XPath 是一门在 XML 文档中查找信息的语言# / :从当前节点的直接子节点中选取。一开始默认是从根节点开始找的# // :从当前节点的子子孙孙中递归选取# /@属性名# /text()# xpath路径可以直接从浏览器复制from lxml import etreehtml=etree.HTML(doc)html=etree.parse(&#x27;search.html&#x27;,etree.HTMLParser())# 1 所有节点# a=html.xpath(&#x27;//*&#x27;)# 2 指定节点（结果为列表）# a=html.xpath(&#x27;//head&#x27;)# 3 子节点，子孙节点# a=html.xpath(&#x27;//div/a&#x27;)# a=html.xpath(&#x27;//body//a&#x27;)# 4 属性匹配# a=html.xpath(&#x27;//body//a[@href=&quot;image1.html&quot;]&#x27;)# 5 文本获取(重要) /text() 取当前标签的文本# a=html.xpath(&#x27;//body//a/text()&#x27;)# 6 属性获取 @href 取当前标签的属性# a=html.xpath(&#x27;//body//a/@href&#x27;)# # 获取某个属性 注意从1 开始取（不是从0）# a=html.xpath(&#x27;//body//a[1]/@href&#x27;)# 7 属性多值匹配# a 标签有多个class类，直接匹配就不可以了，需要用contains。直接匹配需要所有写全所有的class才匹配得上# a=html.xpath(&#x27;//body//a[@class=&quot;li&quot;]&#x27;) 当a只有一个class时可用# a=html.xpath(&#x27;//body//a[contains(@class,&quot;li&quot;)]&#x27;) 当a除了li之外还有其他class时用# a=html.xpath(&#x27;//body//a[contains(@class,&quot;li&quot;)]/text()&#x27;)# 8 多属性匹配# a=html.xpath(&#x27;//body//a[contains(@class,&quot;li&quot;) and @name=&quot;items&quot;]/text()&#x27;) selenium使用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# selenium可以用代码操控浏览器，为了解决requests无法直接执行JavaScript代码的问题 # pip3 install selenium# 浏览器驱动:http://npm.taobao.org/mirrors/chromedriver/# 驱动要跟浏览器版本对应# 下载完解压就是个exe（不同平台的可执行文件）# from selenium import webdriver# import time# # 指定使用跟那个驱动# bro=webdriver.Chrome(executable_path=&#x27;./chromedriver.exe&#x27;) # 得到一个谷歌浏览器对象，# time.sleep(2)# bro.get(&#x27;https://www.baidu.com/&#x27;) # 在地址栏里输入了百度# time.sleep(2)# print(bro.page_source)# time.sleep(2)# bro.close()# ##############选择器（find系列）# ===============所有方法===================# 1、find_element_by_id # 通过id查找控件# 2、find_element_by_link_text # 通过a标签内容找# 3、find_element_by_partial_link_text # 通过a标签内容找，模糊匹配，比如输入一个‘登’就可找到内容为‘登陆’的a标签# 4、find_element_by_tag_name # 标签名# 5、find_element_by_class_name # 类名# 6、find_element_by_name # name属性# 7、find_element_by_css_selector # 通过css选择器# 8、find_element_by_xpath # 通过xpaht选择器# 强调：# 1、find_elements_by_xxx的形式是查找到多个元素，结果为列表find方法被弃用了，新方法是from selenium.webdriver.common.by import Byfind_element(self, by=By.ID, value=‘id_name’)# 获取元素属性# 重点# tag.get_attribute(&#x27;href&#x27;) # 找当前控件 的href属性对的值# tag.text # 获取文本内容# 了解# print(tag.location) # 当前控件在页面位置# print(tag.size) #标签的大小####无界面浏览器（phantomjs）#谷歌浏览器支持不打开页面# from selenium.webdriver.chrome.options import Options# from selenium import webdriver# chrome_options = Options()# chrome_options.add_argument(&#x27;window-size=1920x3000&#x27;) #指定浏览器分辨率# chrome_options.add_argument(&#x27;--disable-gpu&#x27;) #谷歌文档提到需要加上这个属性来规避bug# chrome_options.add_argument(&#x27;--hide-scrollbars&#x27;) #隐藏滚动条, 应对一些特殊页面# chrome_options.add_argument(&#x27;blink-settings=imagesEnabled=false&#x27;) #不加载图片, 提升速度# chrome_options.add_argument(&#x27;--headless&#x27;) #浏览器不提供可视化页面. #bro=webdriver.Chrome(chrome_options=chrome_options,executable_path=&#x27;./chromedriver.exe&#x27;)# bro.get(&#x27;https://www.baidu.com/&#x27;)# print(bro.page_source)# bro.close()######元素交互# tag.send_keys() # 往里面写内容# tag.click() # 点击控件# tag.clear() # 清空控件内容# input_k.send_keys(Keys.ENTER) # 模拟键盘的回车键from selenium.webdriver.common.keys import Keys# bro=webdriver.Chrome(executable_path=&#x27;./chromedriver.exe&#x27;)# bro.implicitly_wait(5) # 隐式等待：找一个控件，如果控件没有加载出来，等待5s，5s内找到控件立马继续执行，5s后找不到就报错 对所有控件生效#####执行js代码# bro.execute_script(&#x27;window.open()&#x27;)# time.sleep(2)# bro.close()#####获取cookie# bro.get_cookies()#### 如何把屏幕拉倒最后（js控制）# bro.execute_script(&#x27;window.scrollTo(0,document.body.offsetHeight)&#x27;) 爬拉勾网职位信息(不同页面cookie不一样，得先拿到cookie)12345678910111213141516171819202122232425262728#https://www.lagou.com/jobs/positionAjax.json?city=%E4%B8%8A%E6%B5%B7&amp;needAddtionalResult=falseimport requests#实际要爬取的urlurl = &#x27;https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false&#x27;payload = &#123; &#x27;first&#x27;: &#x27;true&#x27;, &#x27;pn&#x27;: &#x27;1&#x27;, &#x27;kd&#x27;: &#x27;python&#x27;,&#125;header = &#123; &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36&#x27;, &#x27;Referer&#x27;: &#x27;https://www.lagou.com/jobs/list_python?labelWords=&amp;fromSearch=true&amp;suginput=&#x27;, &#x27;Accept&#x27;: &#x27;application/json, text/javascript, */*; q=0.01&#x27;&#125;#原始的urlurls =&#x27;https://www.lagou.com/jobs/list_python?labelWords=&amp;fromSearch=true&amp;suginput=&#x27;#建立sessions = requests.Session()# 获取搜索页的cookiess.get(urls, headers=header, timeout=3)# 为此次获取的cookiescookie = s.cookies# 获取此次文本response = s.post(url, data=payload, headers=header, cookies=cookie, timeout=5).textprint(response) 自动登录12306123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596破解验证码：先截整个屏，再根据控件的位置和尺寸把验证码部分截图下来，发送到打码平台超级鹰，再根据超级鹰返回的坐标用动作链去点击验证码上的图片即可# 打码平台的使用from selenium import webdriverimport time#pillowfrom PIL import Image# 引入超级鹰from chaojiying import Chaojiying_Clientfrom selenium.webdriver import ActionChainsbro=webdriver.Chrome(executable_path=&#x27;./chromedriver.exe&#x27;)bro.implicitly_wait(10)try: bro.get(&#x27;https://kyfw.12306.cn/otn/resources/login.html&#x27;) bro.maximize_window() # 窗口最大化，全屏 button_z=bro.find_element_by_css_selector(&#x27;.login-hd-account a&#x27;) button_z.click() time.sleep(2) # 截取整个屏幕 bro.save_screenshot(&#x27;./main.png&#x27;) # 验证码的位置和大小 img_t=bro.find_element_by_id(&#x27;J-loginImg&#x27;) print(img_t.size) print(img_t.location) size=img_t.size location=img_t.location img_tu = (int(location[&#x27;x&#x27;]), int(location[&#x27;y&#x27;]), int(location[&#x27;x&#x27;] + size[&#x27;width&#x27;]), int(location[&#x27;y&#x27;] + size[&#x27;height&#x27;])) # # 抠出验证码 # #打开 img = Image.open(&#x27;./main.png&#x27;) # 抠图 fram = img.crop(img_tu) # 截出来的小图 fram.save(&#x27;code.png&#x27;) # 调用超级鹰破解 chaojiying = Chaojiying_Client(&#x27;306334678&#x27;, &#x27;lqz12345&#x27;, &#x27;903641&#x27;) #用户中心&gt;&gt;软件ID 生成一个替换 96001 im = open(&#x27;code.png&#x27;, &#x27;rb&#x27;).read() # print(chaojiying.PostPic(im, 9004)) ## 返回结果如果有多个 260,133|123，233,处理这种格式[[260,133],[123,233]] res=chaojiying.PostPic(im, 9004) print(res) result=res[&#x27;pic_str&#x27;] all_list = [] if &#x27;|&#x27; in result: list_1 = result.split(&#x27;|&#x27;) count_1 = len(list_1) for i in range(count_1): xy_list = [] x = int(list_1[i].split(&#x27;,&#x27;)[0]) y = int(list_1[i].split(&#x27;,&#x27;)[1]) xy_list.append(x) xy_list.append(y) all_list.append(xy_list) else: x = int(result.split(&#x27;,&#x27;)[0]) y = int(result.split(&#x27;,&#x27;)[1]) xy_list = [] xy_list.append(x) xy_list.append(y) all_list.append(xy_list) print(all_list) # 用动作链，点击图片 # [[260,133],[123,233]] for a in all_list: x = a[0] y = a[1] ActionChains(bro).move_to_element_with_offset(img_t, x, y).click().perform() time.sleep(1) username=bro.find_element_by_id(&#x27;J-userName&#x27;) username.send_keys(&#x27;306334678&#x27;) password=bro.find_element_by_id(&#x27;J-password&#x27;) password.send_keys(&#x27;lqz12345&#x27;) time.sleep(3) submit_login=bro.find_element_by_id(&#x27;J-login&#x27;) submit_login.click() time.sleep(3) print(bro.get_cookies()) time.sleep(10) bro.get(&#x27;https://www.12306.cn/index/&#x27;) time.sleep(5)except Exception as e: print(e)finally: bro.close() 自动给抽屉点赞1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from selenium import webdriverimport timeimport requestsbro=webdriver.Chrome(executable_path=&#x27;./chromedriver.exe&#x27;)bro.implicitly_wait(10)bro.get(&#x27;https://dig.chouti.com/&#x27;)login_b=bro.find_element_by_id(&#x27;login_btn&#x27;)print(login_b)login_b.click()username=bro.find_element_by_name(&#x27;phone&#x27;)username.send_keys(&#x27;18953675221&#x27;)password=bro.find_element_by_name(&#x27;password&#x27;)password.send_keys(&#x27;lqz123&#x27;)button=bro.find_element_by_css_selector(&#x27;button.login-btn&#x27;)button.click()# 可能有验证码，手动操作一下。# 程序休眠时，浏览器会给出验证码，我们手动操作一下，输完验证码浏览器就会朝服务器发送请求完成登陆。time.sleep(10)# 等程序结束休眠时，浏览器已经发送完请求完成登陆了，这时就可以拿到cookies了my_cookie=bro.get_cookies() # 列表print(my_cookie)bro.close()# 这个cookie不是一个字典，不能直接给requests使用，需要转一下cookie=&#123;&#125;for item in my_cookie: cookie[item[&#x27;name&#x27;]]=item[&#x27;value&#x27;]headers = &#123; &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27;, &#x27;Referer&#x27;: &#x27;https://dig.chouti.com/&#x27;&#125;# ret = requests.get(&#x27;https://dig.chouti.com/&#x27;,headers=headers)# print(ret.text)# 进入首页无须cookies，直接进入即可ret=requests.get(&#x27;https://dig.chouti.com/top/24hr?_=1596677637670&#x27;,headers=headers)print(ret.json())ll=[]for item in ret.json()[&#x27;data&#x27;]: ll.append(item[&#x27;id&#x27;])print(ll)for id in ll: # 点赞需要登陆才能操作，这时发post请求就需要 cookies了 ret=requests.post(&#x27; https://dig.chouti.com/link/vote&#x27;,headers=headers,cookies=cookie,data=&#123;&#x27;linkId&#x27;:id&#125;) print(ret.text)# 自动评论&#x27;https://dig.chouti.com/comments/create&#x27;&#x27;&#x27;&#x27;content: 说的号linkId: 29829529parentId: 0&#x27;&#x27;&#x27; cookie池讲解(类似代理池)12345# 如何搭建cookie池# selenium写一套（一堆小号），跑起脚本，自动登录，手动参与# 拿到cookie，放到redis中# django搭建一个服务：127.0.0.0/get,随机返回一个cookie# request发送请求爬数据（带着cookie池里的cookie），检测到cookie失效就删除cookie 抓包工具介绍1234如何获得网页的信息# 1 浏览器调试模式# 2 抓包工具：fiddler，charles(自己研究一下)。。原理就是你的数据出去和进来都会经过这个抓包工具，它会给你解析出截获的数据 ScrapyScrapy原理 SPIDERS对应爬虫文件中的parser函数，这个函数yield出url后，url会被封装成REQUESTS对象传到ENGINE ENGINE把1中传过来的requests对象传到schedule中，schedule会把requests放到队列中，安排顺序 schedule调度队列中的requests对象，有序日到ENGINE ENGINE把3中传来的requests通过download中间件传送到download中，由download负责发出请求并接收响应 download接收到响应并把响应封装成response对象通过download中间件传送到ENFGINE ENGINE把5中传来的response对象通过spider中间件送到SPIDERS(即parser)中进行解析 SPIDERS把解析的结果通过SPIDER中间件送到ENGINE，可能是items，也可能是url(封装后成requests) ENGINE判断7中传来的对象是什么，如果是items对象就送到item pipelines中进行存储；如果是requests对象就送到schedule里继续重复3-8 scrapy 介绍1234567891011121314#1 通用的网络爬虫框架,爬虫界的django#2 scrapy执行流程 5大组件 -引擎(EGINE)：大总管，负责控制数据的流向 -调度器(SCHEDULER)：由它来决定下一个要抓取的网址是什么，去重 -下载器(DOWLOADER)：用于下载网页内容, 并将网页内 容返回给EGINE，下载器是建立在twisted这个高效的异步模型上的 -爬虫(SPIDERS):开发人员自定义的类，用来解析responses，并且提取items，或者发送新的请求request -项目管道(ITEM PIPLINES):在items被提取后负责处理它们，主要包括清理、验证、持久化（比如存到数据库）等操作 2大中间件 -爬虫中间件：位于EGINE和SPIDERS之间，主要工作是处理SPIDERS的输入和输出（用的很少） -下载中间件：引擎和下载器之间，加代理，加头，集成selenium # 3 开发者只需要在固定的位置写固定的代码即可（写的最多的spider） scrapy安装（windows，mac，linux）1234567891011#1 pip3 install scrapy（mac，linux）#2 windows上（直接pip install scrapy 80%能成功，成功不了才按下面的走） 1、pip3 install wheel #安装后，便支持通过wheel文件安装软件，wheel文件官网：https://www.lfd.uci.edu/~gohlke/pythonlibs 3、pip3 install lxml 4、pip3 install pyopenssl 5、下载并安装pywin32：https://sourceforge.net/projects/pywin32/files/pywin32/ 6、下载twisted的wheel文件：http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted 7、执行pip3 install 下载目录\\Twisted-17.9.0-cp36-cp36m-win_amd64.whl 8、pip3 install scrapy# 3 安装成功就有scrapy命令 -D:\\Python36\\Scripts\\scrapy.exe 用于创建项目 scrapy 创建项目，创建爬虫，运行爬虫123456789101112131 scrapy startproject 项目名 -scrapy startproject firstscrapy2 创建爬虫 -scrapy genspider 爬虫名 爬虫地址 -scrapy genspider chouti dig.chouti.com -一执行就会在spider文件夹下创建出一个py文件，名字叫chouti3 运行爬虫 -scrapy crawl chouti # 带运行日志 -scrapy crawl chouti --nolog # 不带日志4 支持右键执行爬虫 -在项目路径下新建一个main.py from scrapy.cmdline import execute execute([&#x27;scrapy&#x27;,&#x27;crawl&#x27;,&#x27;chouti&#x27;,&#x27;--nolog&#x27;]) 目录介绍123456789101112# 目录介绍firstscrapy # 项目名字 firstscrapy # 包 -spiders # 所有的爬虫文件放在里面。下面所提到的爬虫指的就是spiders文件夹里的py文件 -baidu.py # 一个个的爬虫（以后基本上都在这写东西） -chouti.py # 函数中的spider参数就是这些文件，其中spider.name就是文件名，可以通过对spider.name的判断实现对不同的爬虫进行不同的处理 -middlewares.py # 中间件（爬虫，下载中间件都写在这） -pipelines.py # 持久化相关写在这（items.py中类的对象） -main.py # 自己加的，右键执行爬虫 -items.py # 一个一个的类， -settings.py # 配置文件 scrapy.cfg # 上线相关 settings介绍1234567891 默认情况，scrapy会去遵循爬虫协议2 修改配置文件参数，强行爬取，不遵循协议 -ROBOTSTXT_OBEY = False3 USER_AGENT = &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27; #换请求头，否则默认的请求头是&#x27;scrapy&#x27;太爬虫了4 LOG_LEVEL=&#x27;ERROR&#x27; # 更改日志级别，只有error信息会被打印出来5 pipelines.py写的类要在settings中注册才有效 ITEM_PIPELINES = &#123; &#x27;firstscrapy.pipelines.ChoutiFilePipeline&#x27;: 300, &#125; scrapy的数据解析（重点）123456789scrapy自带的数据解析只有xpath和css#xpath： -response.xpath(&#x27;//a[contains(@class,&quot;link-title&quot;)]/text()&#x27;).extract() # 取文本 -response.xpath(&#x27;//a[contains(@class,&quot;link-title&quot;)]/@href&#x27;).extract() #取属性#css -response.css(&#x27;.link-title&#x27;).extract() # 取标签 -response.css(&#x27;.link-title::text&#x27;).extract() # 取文本 -response.css(&#x27;.link-title::attr(href)&#x27;).extract_first() # 取属性 scrapy的持久化存储（重点）123456789101112131415161718#1 方案一：parser函数必须返回列表套字典的形式（了解） scrapy crawl chouti -o chouti.csv#2 方案二：高级，pipline item存储（mysql，redis，文件）一般用这种 -在Items.py中写一个类 -在spider中导入，实例化，把数据放进去 item[&#x27;title&#x27;]=title item[&#x27;url&#x27;]=url item[&#x27;photo_url&#x27;]=photo_url yield item -在setting中配置（数字越小，级别越高） ITEM_PIPELINES = &#123; &#x27;firstscrapy.pipelines.ChoutiFilePipeline&#x27;: 300, &#125; -在pipelines.py中写ChoutiFilePipeline -open_spider（开始的时候） -close_spider（结束的时候） -process_item（在这持久化） 自动给抽屉点赞1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from selenium import webdriverimport timeimport requestsbro=webdriver.Chrome(executable_path=&#x27;./chromedriver.exe&#x27;)bro.implicitly_wait(10)bro.get(&#x27;https://dig.chouti.com/&#x27;)login_b=bro.find_element_by_id(&#x27;login_btn&#x27;)print(login_b)login_b.click()username=bro.find_element_by_name(&#x27;phone&#x27;)username.send_keys(&#x27;18953675221&#x27;)password=bro.find_element_by_name(&#x27;password&#x27;)password.send_keys(&#x27;lqz123&#x27;)button=bro.find_element_by_css_selector(&#x27;button.login-btn&#x27;)button.click()# 可能有验证码，手动操作一下。# 程序休眠时，浏览器会给出验证码，我们手动操作一下，输完验证码浏览器就会朝服务器发送请求完成登陆。time.sleep(10)# 等程序结束休眠时，浏览器已经发送完请求完成登陆了，这时就可以拿到cookies了my_cookie=bro.get_cookies() # 列表print(my_cookie)bro.close()# 这个cookie不是一个字典，不能直接给requests使用，需要转一下cookie=&#123;&#125;for item in my_cookie: cookie[item[&#x27;name&#x27;]]=item[&#x27;value&#x27;]headers = &#123; &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#x27;, &#x27;Referer&#x27;: &#x27;https://dig.chouti.com/&#x27;&#125;# ret = requests.get(&#x27;https://dig.chouti.com/&#x27;,headers=headers)# print(ret.text)# 进入首页无须cookies，直接进入即可ret=requests.get(&#x27;https://dig.chouti.com/top/24hr?_=1596677637670&#x27;,headers=headers)print(ret.json())ll=[]for item in ret.json()[&#x27;data&#x27;]: ll.append(item[&#x27;id&#x27;])print(ll)for id in ll: # 点赞需要登陆才能操作，这时发post请求就需要 cookies了 ret=requests.post(&#x27; https://dig.chouti.com/link/vote&#x27;,headers=headers,cookies=cookie,data=&#123;&#x27;linkId&#x27;:id&#125;) print(ret.text)# 自动评论&#x27;https://dig.chouti.com/comments/create&#x27;&#x27;&#x27;&#x27;content: 说的号linkId: 29829529parentId: 0&#x27;&#x27;&#x27; scrapy的请求传参，meta12345本质就是在download中间件中，程序把 Request中的meta参数复制了一份给response中的meta了# 把要传递的数据放到meta中yield Request(urlmeta=&#123;&#x27;item&#x27;:item&#125;)# 在response对象中取出来item=response.meta.get(&#x27;item&#x27;) scrapy的中间件（download中间件）12345678910111213141516171819202122232425262728293031# 1 都写在middlewares.py# 2 爬虫中间件(用得少)# 3 下载中间件# 4 要生效，一定要配置，配置文件# 下载中间件-process_request：返回不同的对象，后续处理不同（可以返回none、request、response、exception这四种对象，scrapy对每一种对象的处理方式不同，可以通过源码了解） # 1 更换请求头 # from scrapy.http.headers import Headers # request.headers[&#x27;User-Agent&#x27;]=&#x27;&#x27; # 2 加cookie ---cookie池 # 假设你你已经搭建好cookie 池了， # print(&#x27;00000--&#x27;,request.cookies) # request.cookies=&#123;&#x27;username&#x27;:&#x27;asdfasdf&#x27;&#125; # 3 加代理，在meta里面加 # print(request.meta) # request.meta[&#x27;download_timeout&#x27;] = 20 # request.meta[&quot;proxy&quot;] = &#x27;http://27.188.62.3:8060&#x27;-process_response：返回不同的对象，后续处理不同- process_exception # 有异常就会走这个函数def process_exception(self, request, exception, spider): print(&#x27;xxxx&#x27;) # 不允许直接改url， 只能重新构造Requests对象返回 # request.url=&#x27;https://www.baidu.com&#x27; from scrapy import Request request=Request(url=&#x27;https://www.baidu.com&#x27;,callback=spider.parser) # callback参数是异步回调参数，表示当结果返回时用哪个函数来处理返回的结果 return request selenium在scrapy中的使用流程12345678910111213141516# 要让当前所有爬虫用的selenium是同一个浏览器。如果在中间件中起浏览器的话就是一个爬虫对应一个浏览器了，那样资源不够的# 1 在爬虫中初始化webdriver对象。这里的爬虫指的是spider文件夹里的py文件 from selenium import webdriver class CnblogSpider(scrapy.Spider): name = &#x27;cnblog&#x27; ... bro=webdriver.Chrome(executable_path=&#x27;../chromedriver.exe&#x27;)# 2 在中间件中使用（process_request） 使用中间件时记得去settings里面注册 spider.bro.get(&#x27;https://dig.chouti.com/&#x27;) response=HtmlResponse(url=&#x27;https://dig.chouti.com/&#x27;,body=spider.bro.page_source.encode(&#x27;utf-8&#x27;),request=request) return response # 3 在爬虫中关闭 def close(self, reason): print(&quot;我结束了&quot;) self.bro.close() 分布式爬虫（scrapy-redis）1234567891011121314151617181920212223分布式爬虫的原理是，在redis中开辟一个共享队列，把要爬取的所有url都放在里面，所有的机器(或者进程)都去共享队列里拿url爬并进行解析，再把解析的结果存到各自的数据库里。如果用的是同一个数据库的表，那么数据也会被拼在一起# 1 pip3 install scrapy-redis# 2 原来继承Spider的类，现在改为继承RedisSpider# 3 不能写start_urls = [&#x27;https:/www.cnblogs.com/&#x27;]，需要改成写redis_key = &#x27;myspider:start_urls&#x27;# 4 setting中配置：# redis的连接REDIS_HOST = &#x27;localhost&#x27; # 主机名REDIS_PORT = 6379 # 端口 # 使用scrapy-redis的去重DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;# 使用scrapy-redis的Scheduler# 分布式爬虫的配置SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;# 持久化的可以配置，也可以不配置。如果进行了配置，那么解析出的数据也会存一份在redis里ITEM_PIPELINES = &#123; &#x27;scrapy_redis.pipelines.RedisPipeline&#x27;: 299 #数字表示优先级，越小优先级越高&#125;# 5现在要让爬虫运行起来，需要去redis中以myspider:start_urls为key，插入一个起始地址：lpush myspider:start_urls https://www.cnblogs.com/ 破解知乎登陆(js逆向和解密)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118#js逆向和解密：有些网站在post数据的时候会对数据用js代码进行加密，这时如果我们相拥request发送提交请求时就要带上加密的数据才行。那我们首先要破解它的加密方式，再把加密方式应用于我们的数据上。破解加密方式就是从网站加密后的结果逆向猜测它加密的方式，可以通过前端控制台的debug调试去猜测。如果猜得出来那还行，如果猜不出来，就只能扣出网页加密模块的js代码再用python中的模块去直接执行js代码得到加密数据了知乎post数据的格式client_id=c3cef7c66a1843f8b3a9e6a1e3160e20&amp;grant_type=password&amp;timestamp=1596702006088&amp;source=com.zhihu.web&amp;signature=eac4a6c461f9edf86ef33ef950c7b6aa426dbb39&amp;username=%2B86liuqingzheng&amp;password=1111111&amp;captcha=&amp;lang=en&amp;utm_source=&amp;ref_source=other_https%3A%2F%2Fwww.zhihu.com%2Fsignin%3Fnext%3D%252F&quot;# 破解知乎登陆import requests #请求解析库import base64 #base64解密加密库，用于base64格式的图片的存取from PIL import Image #图片处理库import hmac #加密库from hashlib import sha1 #加密库import timefrom urllib.parse import urlencode #url编码库import execjs #python调用node.jsfrom http import cookiejar as cookielibclass Spider(): def __init__(self): self.session = requests.session() self.session.cookies = cookielib.LWPCookieJar() #使cookie可以调用save和load方法 self.login_page_url = &#x27;https://www.zhihu.com/signin?next=%2F&#x27; self.login_api = &#x27;https://www.zhihu.com/api/v3/oauth/sign_in&#x27; self.captcha_api = &#x27;https://www.zhihu.com/api/v3/oauth/captcha?lang=en&#x27; self.headers = &#123; &#x27;user-agent&#x27;:&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36 LBBROWSER&#x27;, &#125; self.captcha =&#x27;&#x27; #存验证码 self.signature = &#x27;&#x27; #存签名 # 首次请求获取cookie def get_base_cookie(self): self.session.get(url=self.login_page_url, headers=self.headers) def deal_captcha(self): r = self.session.get(url=self.captcha_api, headers=self.headers) r = r.json() if r.get(&#x27;show_captcha&#x27;): while True: r = self.session.put(url=self.captcha_api, headers=self.headers) img_base64 = r.json().get(&#x27;img_base64&#x27;) with open(&#x27;captcha.png&#x27;, &#x27;wb&#x27;) as f: f.write(base64.b64decode(img_base64)) captcha_img = Image.open(&#x27;captcha.png&#x27;) captcha_img.show() self.captcha = input(&#x27;输入验证码:&#x27;) r = self.session.post(url=self.captcha_api, data=&#123;&#x27;input_text&#x27;: self.captcha&#125;, headers=self.headers) if r.json().get(&#x27;success&#x27;): break def get_signature(self): # 生成加密签名 a = hmac.new(b&#x27;d1b964811afb40118a12068ff74a12f4&#x27;, digestmod=sha1) a.update(b&#x27;password&#x27;) a.update(b&#x27;c3cef7c66a1843f8b3a9e6a1e3160e20&#x27;) a.update(b&#x27;com.zhihu.web&#x27;) a.update(str(int(time.time() * 1000)).encode(&#x27;utf-8&#x27;)) self.signature = a.hexdigest() def post_login_data(self): data = &#123; &#x27;client_id&#x27;: &#x27;c3cef7c66a1843f8b3a9e6a1e3160e20&#x27;, &#x27;grant_type&#x27;: &#x27;password&#x27;, &#x27;timestamp&#x27;: str(int(time.time() * 1000)), &#x27;source&#x27;: &#x27;com.zhihu.web&#x27;, &#x27;signature&#x27;: self.signature, &#x27;username&#x27;: &#x27;+8618953675221&#x27;, &#x27;password&#x27;: &#x27;&#x27;, &#x27;captcha&#x27;: self.captcha, &#x27;lang&#x27;: &#x27;en&#x27;, &#x27;utm_source&#x27;: &#x27;&#x27;, &#x27;ref_source&#x27;: &#x27;other_https://www.zhihu.com/signin?next=%2F&#x27;, &#125; headers = &#123; &#x27;x-zse-83&#x27;: &#x27;3_2.0&#x27;, &#x27;content-type&#x27;: &#x27;application/x-www-form-urlencoded&#x27;, &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36 LBBROWSER&#x27;, &#125; data = urlencode(data) with open(&#x27;zhih.js&#x27;, &#x27;rt&#x27;, encoding=&#x27;utf-8&#x27;) as f: js = execjs.compile(f.read(), cwd=&#x27;node_modules&#x27;) data = js.call(&#x27;b&#x27;, data) r = self.session.post(url=self.login_api, headers=headers, data=data) print(r.text) if r.status_code == 201: self.session.cookies.save(&#x27;mycookie&#x27;) print(&#x27;登录成功&#x27;) else: print(&#x27;登录失败&#x27;) def login(self): self.get_base_cookie() self.deal_captcha() self.get_signature() self.post_login_data()if __name__ == &#x27;__main__&#x27;: zhihu_spider = Spider() zhihu_spider.login() 爬虫的反扒措施123456781 user-agent2 referer3 cookie（访问多了可能封ip，可以用cookie池；有些网站访问首页的cookie和用户的不一样，可以先访问一次拿到cookie）4 频率限制（代理池；times.sleep延迟）5 js加密（扣出js代码，exjs模块执行js代码）6 css加密7 验证码（打码平台），半手动输入验证码8 图片懒加载。有些网站如果你不下滑，下面的图片就不加载，这时这些图片的src存在对应标签的lazy-img属性里，等到需要加载图片时再把lazy-img的值放到src里进行加载","categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"python","slug":"code/python","permalink":"http://example.com/categories/code/python/"},{"name":"爬虫","slug":"code/python/爬虫","permalink":"http://example.com/categories/code/python/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"知识分享","slug":"知识分享","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"}]},{"title":"Django数据库操作","slug":"Django数据库操作","date":"2023-02-06T09:16:46.016Z","updated":"2023-02-06T09:23:37.682Z","comments":true,"path":"2023/02/06/Django数据库操作/","link":"","permalink":"http://example.com/2023/02/06/Django%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C/","excerpt":"","text":"pycharm链接数据库(MySQL)123456三个位置查找数据库相关 右侧上方database 左下方database 配置里面的plugins插件搜索安装 需要提前创建好库 django链接数据库(MySQL)12345678910111213141516171819202122232425262728# 默认用的是sqlite3DATABASES = &#123; &#x27;default&#x27;: &#123; &#x27;ENGINE&#x27;: &#x27;django.db.backends.sqlite3&#x27;, &#x27;NAME&#x27;: os.path.join(BASE_DIR, &#x27;db.sqlite3&#x27;), &#125;&#125;# django链接MySQL 1.第一步配置文件中配置 DATABASES = &#123; &#x27;default&#x27;: &#123; &#x27;ENGINE&#x27;: &#x27;django.db.backends.mysql&#x27;, &#x27;NAME&#x27;: &#x27;day60&#x27;, &#x27;USER&#x27;:&#x27;root&#x27;, &#x27;PASSWORD&#x27;:&#x27;admin123&#x27;, &#x27;HOST&#x27;:&#x27;127.0.0.1&#x27;, &#x27;PORT&#x27;:3306, &#x27;CHARSET&#x27;:&#x27;utf8&#x27; &#125;&#125; 2.代码声明 django默认用的是mysqldb模块链接MySQL 但是该模块的兼容性不好 需要手动改为用pymysql链接 # 在项目名下的init或者任意的应用名下的init文件中书写以下代码都可以 import pymysql pymysql.install_as_MySQLdb() Django ORM、表的建立1234567891011121314151617181920212223242526272829303132333435363738394041424344&quot;&quot;&quot;ORM. 对象关系映射作用:能够让一个不用sql语句的小白也能够通过python 面向对象的代码简单快捷的操作数据库不足之处:封装程度太高 有时候sql语句的效率偏低 需要你自己写SQL语句应用下面的models.py文件&quot;&quot;&quot;# 1 先去models.py中书写一个类 class User(models.Model):#这里得继承，别忘了 # id int primary_key auto_increment id = models.AutoField(primary_key=True) # username varchar(32) username = models.CharField(max_length=32) # password int password = models.IntegerField()*************************# 2 数据库迁移命令*************************python3 manage.py makemigrations 将操作记录记录到migrations文件夹中python3 manage.py migrate 将操作真正的同步到数据库中# 只要你修改了models.py中跟数据库相关的代码 就必须重新执行上述的两条命令******************************************************************class User(models.Model): # id int primary_key auto_increment id = models.AutoField(primary_key=True,verbose_name=&#x27;主键&#x27;) # username varchar(32) username = models.CharField(max_length=32,verbose_name=&#x27;用户名&#x27;) &quot;&quot;&quot; CharField必须要指定max_length参数 不指定会直接报错 verbose_name该参数是所有字段都有的 就是用来对字段的解释 &quot;&quot;&quot; # password int password = models.IntegerField(verbose_name=&#x27;密码&#x27;)class Author(models.Model): # 由于一张表中必须要有一个主键字段 并且一般情况下都叫id字段 # 所以orm当你不定义主键字段的时候 orm会自动帮你创建一个名为id主键字段 # 也就意味着 后续我们在创建模型表的时候如果主键字段名没有额外的叫法 那么主键字段可以省略不写 # username varchar(32) username = models.CharField(max_length=32) # password int password = models.IntegerField() 字段的增删改查12345678910111213141516171819# 字段的增加1.可以在终端内直接给出默认值2.该字段可以为空 info = models.CharField(max_length=32,verbose_name=&#x27;个人简介&#x27;,null=True)3.直接给字段设置默认值 hobby = models.CharField(max_length=32,verbose_name=&#x27;兴趣爱好&#x27;,default=&#x27;study&#x27;) # 字段的修改 直接修改代码然后执行数据库迁移的两条命令即可# 字段的删 直接注释对应的字段然后执行数据库迁移的两条命令即可 执行完毕之后字段对应的数据也都没有了 &quot;&quot;&quot;在操作models.py的时候一定要细心 千万不要注释一些字段 执行迁移命令之前最好先检查一下自己写的代码&quot;&quot;&quot; 数据的增删改查12345678910111213141516171819202122232425262728293031323334353637# 查res = models.User.objects.filter(username=username)&quot;&quot;&quot;返回值你先看成是列表套数据对象的格式它也支持索引取值 切片操作 但是不支持负数索引它也不推荐你使用索引的方式取值user_obj = models.User.objects.filter(username=username).first()取到的是与记录相关的对象的列表的第一个元素，可以通过user_obj.username 或 user_obj.password (user_obj.字段名)取到对应的属性&quot;&quot;&quot;filter括号内可以携带多个参数 参数与参数之间默认是and关系你可以把filter联想成where记忆如果符合条件的有多条记录，那么user_obj列表中的元素就有多个，可以通过索引取值# 增from app01 import modelsres = models.User.objects.create(username=username,password=password)# 返回值就是当前被创建的对象本身# 第二种增加user_obj = models.User(username=username,password=password)user_obj.save() # 保存数据# 修改数据方式1models.User.objects.filter(id=edit_id).update(username=username,password=password)将filter查询出来的列表中所有的对象全部更新 批量更新操作只修改被修改的字段# 修改数据方式2edit_obj.username = usernameedit_obj.password= passwordedit_obj.save()上述方法当字段特别多的时候效率会非常的低，因为它从头到尾将数据的所有字段全部更新一遍，无论该字段是否被修改#删models.User.objects.filter(id=delete_id).delete() django orm中如何创建表关系1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&quot;&quot;&quot;表与表之间的关系 一对多 多对多 一对一 没有关系判断表关系的方法:换位思考若1个a对应多个b，1个b对应1个a，那么a和b就是1对多，且多的一方是b若1个a对应多个b，1个b对应多个a，那么a和b就是多对多若1个a对应1个b，1个b对应1个a，那么a和b就是1对1&quot;&quot;&quot;图书表出版社表作者表作者详情表&quot;&quot;&quot;图书和出版社是一对多的关系 外键字段建在多的那一方 book图书和作者是多对多的关系 需要创建第三张表来专门存储作者与作者详情表是一对一&quot;&quot;&quot;models.py的代码如下from django.db import models# 创建表关系 先将基表创建出来 然后再添加外键字段class Book(models.Model): title = models.CharField(max_length=32) price = models.DecimalField(max_digits=8,decimal_places=2)# 总共八位 小数点后面占两位 &quot;&quot;&quot; 图书和出版社是一对多 并且书是多的一方 所以外键字段放在书表里面 &quot;&quot;&quot; publish = models.ForeignKey(to=&#x27;Publish&#x27;) # 默认就是与出版社表的主键字段做外键关联 &quot;&quot;&quot; 如果字段对应的是ForeignKey 那么会orm会自动在字段的后面加_id 如果你加了_id那么orm还是会在后面继续加_id，变成name_id_id了，所以在定义ForeignKey的时候就不要自己加_id &quot;&quot;&quot; &quot;&quot;&quot; 图书和作者是多对多的关系 外键字段建在任意一方均可 但是推荐你建在查询频率较高的一方 &quot;&quot;&quot; authors = models.ManyToManyField(to=&#x27;Author&#x27;) &quot;&quot;&quot; authors是一个虚拟字段 主要是用来告诉orm 书籍表和作者表是多对多关系 让orm自动帮你创建第三张关系表 &quot;&quot;&quot;class Publish(models.Model): name = models.CharField(max_length=32) addr = models.CharField(max_length=32)class Author(models.Model): name = models.CharField(max_length=32) age = models.IntegerField() &quot;&quot;&quot; 作者与作者详情是一对一的关系 外键字段建在任意一方都可以 但是推荐你建在查询频率较高的表中 &quot;&quot;&quot; author_detail = models.OneToOneField(to=&#x27;AuthorDetail&#x27;) &quot;&quot;&quot; OneToOneField也会自动给字段加_id后缀 &quot;&quot;&quot;class AuthorDetail(models.Model): phone = models.BigIntegerField() # 或者直接字符类型 addr = models.CharField(max_length=32)&quot;&quot;&quot; orm中如何定义三种关系 publish = models.ForeignKey(to=&#x27;Publish&#x27;) # 默认就是与出版社表的主键字段做外键关联 authors = models.ManyToManyField(to=&#x27;Author&#x27;) author_detail = models.OneToOneField(to=&#x27;AuthorDetail&#x27;) ForeignKey、OneToOneField会自动在字段后面加_id后缀&quot;&quot;&quot;# 在django1.X版本中外键默认都是级联更新删除的","categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"python","slug":"code/python","permalink":"http://example.com/categories/code/python/"},{"name":"django","slug":"code/python/django","permalink":"http://example.com/categories/code/python/django/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"知识分享","slug":"知识分享","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"}]},{"title":"Django项目的搭建和配置","slug":"Django项目的搭建","date":"2023-02-05T07:46:17.205Z","updated":"2023-02-06T09:23:43.056Z","comments":true,"path":"2023/02/05/Django项目的搭建/","link":"","permalink":"http://example.com/2023/02/05/Django%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%90%AD%E5%BB%BA/","excerpt":"","text":"Django创建项目 创建应用：pycharm的terminal中运行start manage.py start app01 以下操作都是在项目名文件夹下的settings.py操作的 注册应用： 123456789INSTALLED_APPS = [ &#x27;django.contrib.admin&#x27;, &#x27;django.contrib.auth&#x27;, &#x27;django.contrib.contenttypes&#x27;, &#x27;django.contrib.sessions&#x27;, &#x27;django.contrib.messages&#x27;, &#x27;django.contrib.staticfiles&#x27;, &#x27;app01&#x27;, # INSTALLED_APPS列表里加上&#x27;app01&#x27;] 写入templates路径： 1234567TEMPLATES = [ &#123; &#x27;BACKEND&#x27;: &#x27;django.template.backends.django.DjangoTemplates&#x27;, &#x27;DIRS&#x27;: [os.path.join(BASE_DIR, &#x27;templates&#x27;)], &#125;如果报错：Templates doesn&#x27;t exist. 那么可能是因为settings里面的Templates里的DIRS列表里没有写入template的路径 连接数据库配置 1234567891011DATABASES = &#123; &#x27;default&#x27;: &#123; &#x27;ENGINE&#x27;: &#x27;django.db.backends.mysql&#x27;, &#x27;HOST&#x27;: &#x27;127.0.0.1&#x27;, &#x27;PORT&#x27;: 3306, &#x27;USER&#x27;: &#x27;root&#x27;, &#x27;PASSWORD&#x27;: 123, &#x27;NAME&#x27;: &#x27;bbs&#x27;, &#x27;CHARSET&#x27;: &#x27;utf8&#x27; &#125;&#125; 设置静态文件读取路径，方便使用bootsrap和其他组件样式： 123STATICFILES_DIRS = [ os.path.join(BASE_DIR, &#x27;static&#x27;)] 在app下的随便一个init文件配置数据库： 12import pymysqlpymysql.install_as_MySQLdb() Django文件介绍1234567891011121314-mysite项目文件夹 --mysite文件夹 --settings.py 配置文件 --urls.py 路由与视图函数对应关系(路由层) --wsgi.py wsgiref模块(一般不考虑) --manage.py django的入口文件 --db.sqlite3 django自带的sqlite3数据库(小型数据库 功能不是很多还有bug) --app01文件夹 --admin.py django后台管理 --apps.py 注册使用 --migrations文件夹 数据库迁移记录 --models.py 数据库相关的 模型类(orm) --tests.py 测试文件 --views.py 视图函数(视图层) django小白必会三板斧123456789101112&quot;&quot;&quot;HttpResponse 返回字符串类型的数据render 返回html文件的redirect 重定向，想指定的url发送请求的 return redirect(&#x27;https://www.mzitu.com/&#x27;) return redirect(&#x27;/home/&#x27;)&quot;&quot;&quot; 静态文件配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455我们将html文件默认都放在templates文件夹下我们将网站所使用的静态文件默认都放在static文件夹下静态文件 js文件 css文件 网站用到的图片文件 第三方前端框架 ... 拿来就可以直接使用的文件 # django默认是不会自动帮你创建static文件夹 需要你自己手动创建一般情况下我们在static文件夹内还会做进一步的划分处理 -static --js --css --img 其他第三方文件# 静态文件配置在配置文件settings里面STATIC_URL = &#x27;/static/&#x27; # 类似于访问静态文件的令牌如果你想要访问静态文件 你就必须以static开头，比如/static/bootstrap-3.3.7-dist/js/bootstrap.min.js有了/static/令牌，才能从列表里面的路径从上往下依次查找指定文件 bootstrap-3.3.7-dist/js/bootstrap.min.js 列表里面的路径都没有指定文件才会报错&quot;&quot;&quot;# 静态文件配置STATICFILES_DIRS = [ os.path.join(BASE_DIR,&#x27;static&#x27;), 因为bootstrap相关文件在static里，所以可以被访问到 os.path.join(BASE_DIR,&#x27;static1&#x27;), os.path.join(BASE_DIR,&#x27;static2&#x27;),]&quot;&quot;&quot;# 静态文件动态解析，通过下列代码引入文件，即使令牌改了也不会影响文件的导入 &#123;% load static %&#125; &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;% static &#x27;bootstrap-3.3.7-dist/css/bootstrap.min.css&#x27; %&#125;&quot;&gt; &lt;script src=&quot;&#123;% static &#x27;bootstrap-3.3.7-dist/js/bootstrap.min.js&#x27; %&#125;&quot;&gt;&lt;/script&gt;# 在前期我们使用django提交post请求的时候，如果不采用csrf认证，需要取配置文件中注释掉一行代码MIDDLEWARE = [ &#x27;django.middleware.security.SecurityMiddleware&#x27;, &#x27;django.contrib.sessions.middleware.SessionMiddleware&#x27;, &#x27;django.middleware.common.CommonMiddleware&#x27;, # &#x27;django.middleware.csrf.CsrfViewMiddleware&#x27;, 注释掉这一行 &#x27; django.contrib.auth.middleware.AuthenticationMiddleware&#x27;, &#x27;django.contrib.messages.middleware.MessageMiddleware&#x27;, &#x27;django.middleware.clickjacking.XFrameOptionsMiddleware&#x27;,]","categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"python","slug":"code/python","permalink":"http://example.com/categories/code/python/"},{"name":"django","slug":"code/python/django","permalink":"http://example.com/categories/code/python/django/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"知识分享","slug":"知识分享","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"}]},{"title":"环境变量的配置和作用","slug":"环境变量的配置和作用","date":"2023-02-05T07:02:40.643Z","updated":"2023-02-05T07:02:40.760Z","comments":true,"path":"2023/02/05/环境变量的配置和作用/","link":"","permalink":"http://example.com/2023/02/05/%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E7%9A%84%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%9C%E7%94%A8/","excerpt":"","text":"环境变量的作用：Windows中如果你要在命令行中执行某个文件，那么系统会先在目前所在的文件夹中检索看这个文件在不在，在的话就执行；不在的话就在Windows的环境变量(是一个存放着各种文件夹路径的列表)中存放的文件夹下去检索目标文件，找到就执行，还找不到就说找不到了。 重复文件名问题：系统在环境变量存放的文件夹中检索文件时，会按照列表中从上到下的顺序检索文件夹，所以如果2个文件夹里有同名文件，那么系统只会找到存放在上面的那个文件夹的文件执行。解决方案是：各复制一份这个重复名字的文件的副本，再分别进行重命名(直接改名字而不复制可能会受到限制不给重命名) 环境变量的设置：我的电脑-》属性-》高级系统设置-》环境变量-》添加新的文件夹路径(比如C:\\)","categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"windows","slug":"code/windows","permalink":"http://example.com/categories/code/windows/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"知识分享","slug":"知识分享","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"}]},{"title":"Python内置函数","slug":"Python内置函数","date":"2023-02-05T06:58:05.812Z","updated":"2023-02-05T06:58:05.925Z","comments":true,"path":"2023/02/05/Python内置函数/","link":"","permalink":"http://example.com/2023/02/05/Python%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0/","excerpt":"","text":"enumerate枚举123456789# enumerate枚举test_list = [1, 3, 5, 7, 9]for i, j in enumerate(test_list): print(i, j, sep=&#x27;\\t&#x27;) # i对应索引，j对应列表的内容。即可以取到索引和内容0 11 32 53 74 9 eval执行字符串中的表达式1234x = eval(&#x27;&#123;1, 3, 4&#125;&#x27;)print(type(x)) #dictprint(eval(&#x27;1+9&#x27;)) #10eval(&#x27;print(&quot;hahaha&quot;)&#x27;) #hahaha isinstance(obj, cls)判断obj是不是cls的实例12print(isinstance([1,2], list))print(isinstance([1,2], set)) zip(x, y)123456789101112131415#zip(x, y)拉链函数，将x，y索引相同的两个值组成一个元组，返回这些元组的列表x = [1, 2, 3, 4]y = (11, 22, 33, 44)print(type(zip(x, y)))#&lt;class &#x27;zip&#x27;&gt;for i, j in zip(x, y): #这里的i，j是自动解包的结果 print(i, j, sep=&#x27;\\t&#x27;)t = list(zip(x, y))print(t)&lt;class &#x27;zip&#x27;&gt;1 112 223 334 44[(1, 11), (2, 22), (3, 33), (4, 44)] 进制转换123bin(111)oct(111)hex(111) ascii字符12chr(65)ord(&#x27;a&#x27;)","categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"python","slug":"code/python","permalink":"http://example.com/categories/code/python/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"知识分享","slug":"知识分享","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"}]},{"title":"Python常用模块","slug":"Python常用模块","date":"2023-02-05T06:57:20.051Z","updated":"2023-02-05T06:57:20.158Z","comments":true,"path":"2023/02/05/Python常用模块/","link":"","permalink":"http://example.com/2023/02/05/Python%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97/","excerpt":"","text":"hashlib123456import hashlibm=hashlib.md5()m.update(&#x27;hello&#x27;.encode(&#x27;utf-8&#x27;))m.update(&#x27;world&#x27;.encode(&#x27;utf-8&#x27;))res=m.hexdigest() # &#x27;helloworld&#x27;的加密字符串 序列化模块(Json pickle)123456789101112131415161718序列化得到结果=&gt;特定的格式的内容 有两种用途1、可用于存储=》用于存档 采用一种专用的格式=》pickle只有python可以识别2、传输给其他平台使用=》跨平台数据交互 采用一种通用、能够被所有语言识别的格式=》json# 序列化json_res=json.dumps([1,&#x27;aaa&#x27;,True,False])print(json_res,type(json_res)) # &quot;[1, &quot;aaa&quot;, true, false]&quot;， str# 反序列化l=json.loads(json_res)将序列化的结果写入文件的简单方法with open(&#x27;test.json&#x27;,mode=&#x27;wt&#x27;,encoding=&#x27;utf-8&#x27;) as f: json.dump([1,&#x27;aaa&#x27;,True,False],f) # 普通的dumps方法去掉s而已 res = pickle.dumps([1.2, True, &#x27;acv&#x27;, [11, 2]])#pickle序列化后的类型是byteres2 = pickle.loads(res)with open(&#x27;test.pickle&#x27;, &#x27;rb&#x27;) as f: res = pickle.load(f) os123456789# 获取某一个文件夹下所有的子文件以及子文件夹的名字,只找最外面的1层# res=os.listdir(&#x27;.&#x27;)# os.remove() 删除一个文件# os.rename(&quot;oldname&quot;,&quot;newname&quot;) 重命名文件/目录os.path.isfile(r&#x27;aaa&#x27;)os.path.isdir(r&#x27;aaa&#x27;)os.path.join(&#x27;a&#x27;,&#x27;/&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;)os.path.dirname(path) #获取path的目录路径__file__ #获取当前文件的绝对路径 random模块1234567random.random() #(0,1)----floatrandom.uniform(1, 3) # (1, 3)的小数random.randint(1, 3) # [1,3]random.randrange(1, 3) # [1,3)的整数random.choice([111, &#x27;aaa&#x27;, [4, 5]]) #列表中的三个元素取一个random.seed(10) #在每次用到随机值时可以通过设定seed来指定随机数 sys模块12345命令行中敲：python3.8 run.py 1 2 3 即运行当前目录下的run.py文件，传如参数1 2 3# sys.argv获取的是命令行中解释器后参数值sys.argv的值为[1, 2, 3]这个命令通常在用命令行调用python程序中使用 time模块123456789101112131415161718192021222324252627# 时间分为三种格式：# 1、时间戳：从1970年到现在经过的秒数# 作用：用于时间间隔的计算# print(time.time())# 2、按照某种格式显示的时间：2020-03-30 11:11:11# 作用：用于展示时间# print(time.strftime(&#x27;%Y-%m-%d %H:%M:%S %p&#x27;))# print(time.strftime(&#x27;%Y-%m-%d %X&#x27;))# 3、结构化的时间# 作用：用于单独获取时间的某一部分# res=time.localtime()# print(res) #time.struct_time(tm_year=2023, tm_mon=1, tm_mday=29, tm_hour=10, tm_min=47, tm_sec=45, tm_wday=6, tm_yday=29, tm_isdst=0)# print(res.tm_year)!!!真正需要掌握的只有一条：format string&lt;------&gt;timestamp&#x27;1988-03-03 11:11:11&#x27;+7format string---&gt;struct_time---&gt;timestampstruct_time=time.strptime(&#x27;1988-03-03 11:11:11&#x27;,&#x27;%Y-%m-%d %H:%M:%S&#x27;)timestamp=time.mktime(struct_time)+7*86400 # 时间戳加7天，可用于会员过期时间print(timestamp)format string&lt;---struct_time&lt;---timestampres=time.strftime(&#x27;%Y-%m-%d %X&#x27;,time.localtime(timestamp))print(res)","categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"python","slug":"code/python","permalink":"http://example.com/categories/code/python/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"知识分享","slug":"知识分享","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"}]},{"title":"包和模块的导入","slug":"包和模块的导入","date":"2023-02-05T06:56:03.637Z","updated":"2023-02-05T06:56:03.745Z","comments":true,"path":"2023/02/05/包和模块的导入/","link":"","permalink":"http://example.com/2023/02/05/%E5%8C%85%E5%92%8C%E6%A8%A1%E5%9D%97%E7%9A%84%E5%AF%BC%E5%85%A5/","excerpt":"","text":"包的导入12345678910111213141516171819202122232425262728293031323334&#x27;&#x27;&#x27;1、包就是一个包含有__init__.py文件的文件夹2、为何要有包 包的本质是模块的一种形式，包是用来被当做模块导入&#x27;&#x27;&#x27;导入包实际的运行情况#1、产生一个名称空间#2、运行包下的__init__.py文件（如果导入的是模块就是运行模块名.py文件），将运行过程中产生的名字都丢到1的名称空间中#3、在当前执行文件的名称空间中拿到一个名字‘模块名’，‘模块名’指向1的名称空间# from mmm import x# 环境变量是以执行文件为基准的，所有的被导入的模块或者说后续的其他文件引用的sys.path都是参照执行文件的sys.path，即path的第一个元素都是执行文件所在的文件夹的路径# 强调：# 1.关于包相关的导入语句也分为import和from ... import ...两种，但是无论哪种，无论在什么位置，在导入时都必须遵循一个原则：# 凡是在导入时带点的，点的左边都必须是一个包，否则非法。# 可以带有一连串的点，如import 顶级包.子包.子模块,但都必须遵循这个原则。但对于导入后，在使用时就没有这种限制了，点的左边可以是包,模块，函数，类(它们都可以用点的方式调用自己的属性)。# 例如：# from a.b.c.d.e.f import xxx# import a.b.c.d.e.f# 其中a、b、c、d、e 都必须是包# 2、包A和包B下有同名模块也不会冲突，如A.a与B.a来自俩个命名空间## 3、import导入文件时，产生名称空间中的名字来源于文件，# import 包，产生的名称空间的名字同样来源于文件，即包下的__init__.py，导入包本质就是在导入该文件相对导入：.表示当前文件所在文件夹。..表示上一层文件夹。相对导入一般用于包内模块的相互引用from .mm import f1表示导入位于当前文件所在文件夹的mm.py中的函数f1 模块操作(包的操作同理)： 导入模块： a) 全部导入：import moduleName b) 从模块种导入指定函数：from module impore func1(,func2….) Python在查找模块导入时，遵循以下查找顺序： a) 先查找内存中已经加载好的模块 b) 再查找python的内置模块 c) 最后查找sys.path路径中包含的模块 对system..path的操作： a) sys.path是一个列表，它的第一个路径是当前执行文件所在的文件夹 b) 如果要将不存在于该文件夹的模块导入，要将该模块的路径添加到system.path。 如将func3导入文件： import sys sys.path.append(“func3的路径比如桌面：C:\\Users\\Desktop”)#导入后，python会在你导入func3时在该路径下查找func3的文件 import func3","categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"python","slug":"code/python","permalink":"http://example.com/categories/code/python/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"知识分享","slug":"知识分享","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"}]},{"title":"Python魔法方法","slug":"Python魔法方法","date":"2023-02-05T06:54:31.563Z","updated":"2023-02-05T06:54:31.673Z","comments":true,"path":"2023/02/05/Python魔法方法/","link":"","permalink":"http://example.com/2023/02/05/Python%E9%AD%94%E6%B3%95%E6%96%B9%E6%B3%95/","excerpt":"","text":"魔法方法 魔法方法就是满足了特定条件会自动触发的方法 12345678# __init__：类实例化会触发# __str__:打印对象会触发# __call__:对象()触发类的__call__()。让对象可以像函数一样被调用。类也是对象，类(),类的实例化过程调用元类的__call__。# __new__:在类实例化会触发，它比__init__早,创造对象用的，有点复杂# __del__:del 对象，对象回收的时候触发# __setattr__,__getattr__:(.拦截方法)，当对象.属性触发--&gt;赋值会调用setattr，取值只有引发AttributeError才会调用getattr# __getitem__,__setitem__:([]拦截)，当对象[属性]触发--&gt;赋值会调用setitem，取值会调用getitem# __enter__和__exit__ 上下文管理器 setattr，getattr，setitem，getitem演示1234567891011121314# class Person:# def __init__(self,name):# self.name=name# def __setitem__(self, key, value):# setattr(self,key,value) #反射# def __getitem__(self, item):# return getattr(self,item) # 反射取值## p=Person(&#x27;czw&#x27;)# # p.name=&#x27;ppp&#x27;# p[&#x27;name&#x27;]=10 # 如何实现 重写__setitem__魔法方法# # print(p.name)## print(p[&#x27;name&#x27;]) _eq_12345678910111213141516171819class A: def __init__(self,x,y): self.x = x self.y = y def __eq__(self,obj): # 打印出比较的第二个对象的x值 print(obj.x) if self.x +self.y == obj.x+obj.y: return True else: return Falsea=A(1,2)b=A(99,3)print(a==b)print(a==&#x27;ddd&#x27;) # 当执行==s时，会触发__eq__的执行，并且把b传进去，就是object# ==后只要是对象，就可以传进去，就是object","categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"python","slug":"code/python","permalink":"http://example.com/categories/code/python/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"知识分享","slug":"知识分享","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"}]},{"title":"字符编码","slug":"编码","date":"2023-02-05T06:50:43.850Z","updated":"2023-02-05T06:52:05.123Z","comments":true,"path":"2023/02/05/编码/","link":"","permalink":"http://example.com/2023/02/05/%E7%BC%96%E7%A0%81/","excerpt":"","text":"字符编码 ascii码是一开始美国采用的编码，只有1个字节的大小，表示256种字符，为字母和标点符号 gbk是中文编码 unicode是万国码，但它只做了字符和数字的对应关系，而没有规定存储方式，utf-8就是unicode编码的一种存储方式，即可变长的存储，简单字符用少字节，复杂字符用多字节存储 编码和解码123456789101112131415编码和解码是str和bytes之间的转换bytes.decode(encoding=&#x27;utf-8&#x27;, errors=&#x27;strict&#x27;) Return the bytes decoded to a str.str.encode(encoding=&#x27;utf-8&#x27;, errors=&#x27;strict&#x27;) Return the string encoded to bytes.a = &#x27;上&#x27;# python默认用unicode编码b = a.encode(&#x27;utf-8&#x27;) #用utf-8的编码方式： utf-8的str ==&gt; byteprint(b, type(b))c = b.decode(&#x27;utf-8&#x27;) #用utf-8的解码方式： byte ==&gt; utf-8的strprint(c, type(c))# b&#x27;\\xe4\\xb8\\x8a&#x27; &lt;class &#x27;bytes&#x27;&gt;# 上 &lt;class &#x27;str&#x27;&gt;","categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"python","slug":"code/python","permalink":"http://example.com/categories/code/python/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"知识分享","slug":"知识分享","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"}]},{"title":"深浅拷贝","slug":"深浅拷贝","date":"2023-02-05T06:32:01.099Z","updated":"2023-02-05T06:48:55.610Z","comments":true,"path":"2023/02/05/深浅拷贝/","link":"","permalink":"http://example.com/2023/02/05/%E6%B7%B1%E6%B5%85%E6%8B%B7%E8%B4%9D/","excerpt":"","text":"python不同类型的变量为何可以互相赋值python的一切变量都是指着一个地址，由于地址(也可理解为指针)的大小和类型(都是无符号整数)都是一样的，所以不同类型的变量可以相互赋值，本质上就是地址之间的赋值。 浅拷贝浅拷贝就是复制一份原来地址指向的第一块空间，并给一个新地址指向那份空间的副本，把新地址赋值给新变量。由于浅拷贝只是复制了地址指向的第一块空间，所以如果这个空间存的是不可变对象，那就是真正地拷贝了(即修改不会影响原变量的内容)；但如果存的是可变对象的引用，那么改变了这个对象的内容会影响到原变量的内容的 如果浅拷贝的内容是不可变类型，那么新变量指向的地址和原变量是一样的，因为不可变类型是常量，在系统中只要存一份就可以了。如果是可变类型，就会申请一个新地址，指向原变量的顶层引用的副本 函数传参就是传一个浅拷贝过去 深拷贝深拷贝是递归拷贝，就是申请一个新地址，并把原变量的所有内容都拷贝一份","categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"python","slug":"code/python","permalink":"http://example.com/categories/code/python/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"知识分享","slug":"知识分享","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"}]},{"title":"Python文件操作","slug":"文件操作","date":"2023-02-05T06:28:21.074Z","updated":"2023-02-05T06:48:46.605Z","comments":true,"path":"2023/02/05/文件操作/","link":"","permalink":"http://example.com/2023/02/05/%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/","excerpt":"","text":"编码和解码123456789101112131415编码和解码是str和bytes之间的转换bytes.decode(encoding=&#x27;utf-8&#x27;, errors=&#x27;strict&#x27;) Return the bytes decoded to a str.str.encode(encoding=&#x27;utf-8&#x27;, errors=&#x27;strict&#x27;) Return the string encoded to bytes.a = &#x27;上&#x27;# python默认用unicode编码b = a.encode(&#x27;utf-8&#x27;) #用utf-8的编码方式： utf-8的str ==&gt; byteprint(b, type(b))c = b.decode(&#x27;utf-8&#x27;) #用utf-8的解码方式： byte ==&gt; utf-8的strprint(c, type(c))# b&#x27;\\xe4\\xb8\\x8a&#x27; &lt;class &#x27;bytes&#x27;&gt;# 上 &lt;class &#x27;str&#x27;&gt; 文件操作1234567891011121314151617181920212223242526272829303132333435363738394041424344路径问题：# 解决方案一：推荐。用Python的raw字符串避免所有转义# open(r&#x27;C:\\a.txt\\nb\\c\\d.txt&#x27;)# 解决方案二：用/代替\\# open(&#x27;C:/a.txt/nb/c/d.txt&#x27;)with上下文管理，可以自动在最后执行f.close()关闭文件with open(&#x27;a.txt&#x27;,mode=&#x27;rt&#x27;) as f1,\\ #用\\来完成一条语句的换行 open(&#x27;b.txt&#x27;,mode=&#x27;rt&#x27;) as f2: 指定字符编码强调：t和b不能单独使用，必须跟r/w/a连用t文本（默认的模式） 1、读写都以str（unicode）为单位的 2、文本文件 3、必须指定encoding=&#x27;utf-8&#x27;# 没有指定encoding参数操作系统会使用自己默认的编码# linux系统默认utf-8# windows系统默认gbkwith open(&#x27;c.txt&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f: 模式详解1、r（默认的操作模式）：只读模式，当文件不存在时报错，当文件存在时文件指针跳到开始位置2、w：只写模式，当文件不存在时会创建空文件，当文件存在会清空文件，指针位于开始位置 2.1、在以w模式打开文件没有关闭的情况下，连续写入，新的内容总是跟在旧的之后 2.2、如果重新以w模式打开文件，则会清空文件内容3、a：只追加写，在文件不存在时会创建空文件，在文件存在时文件指针会直接调到末尾t与b模式的区别：1、在操作纯文本文件方面t模式帮我们省去了编码与解码的环节(内部帮我们编码和解码)，b模式则需要手动编码与解码，所以此时t模式更为方便2、针对非文本文件（如图片、视频、音频等）只能使用b模式在读取文件时，为防止文件过大导致内存不够，通常用for循环读取with open(r&#x27;&#123;&#125;&#x27;.format(src_file),mode=&#x27;rb&#x27;) as f1,\\ open(r&#x27;&#123;&#125;&#x27;.format(dst_file),mode=&#x27;wb&#x27;) as f2: for line in f1: f2.write(line) 文本转二进制看《编码和解码》模块","categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"python","slug":"code/python","permalink":"http://example.com/categories/code/python/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"知识分享","slug":"知识分享","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"}]},{"title":"数据类型","slug":"数据类型","date":"2023-02-05T06:16:07.882Z","updated":"2023-02-05T06:48:51.118Z","comments":true,"path":"2023/02/05/数据类型/","link":"","permalink":"http://example.com/2023/02/05/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","excerpt":"","text":"字符串格式化输出12345678910111213141516171819202122232425262728293031323334353637str.format:兼容性好# 按照位置传值# res=&#x27;我的名字是 &#123;&#125; 我的年龄是 &#123;&#125;&#x27;.format(&#x27;czw&#x27;,18)# res=&#x27;我的名字是 &#123;0&#125;&#123;0&#125;&#123;0&#125; 我的年龄是 &#123;1&#125;&#123;1&#125;&#x27;.format(&#x27;czw&#x27;,18)# 打破位置的限制，按照key=value传值# res=&quot;我的名字是 &#123;name&#125; 我的年龄是 &#123;age&#125;&quot;.format(age=18,name=&#x27;czw&#x27;)填充与格式化!!先取到值,然后在冒号后设定填充格式：[填充字符][对齐方式][宽度]!!# *&lt;10：左对齐，总共10个字符，不够的用*号填充print(&#x27;&#123;0:*&lt;10&#125;&#x27;.format(&#x27;开始执行&#x27;)) # 开始执行******# *&gt;10：右对齐，总共10个字符，不够的用*号填充print(&#x27;&#123;0:*&gt;10&#125;&#x27;.format(&#x27;开始执行&#x27;)) # ******开始执行# *^10：居中显示，总共10个字符，不够的用*号填充print(&#x27;&#123;0:*^10&#125;&#x27;.format(&#x27;开始执行&#x27;)) # ***开始执行***精度与进制print(&#x27;&#123;salary:.3f&#125;&#x27;.format(salary=1232132.12351)) #精确到小数点后3位，四舍五入，结果为：1232132.124print(&#x27;&#123;0:b&#125;&#x27;.format(123)) # 转成二进制，结果为：1111011print(&#x27;&#123;0:o&#125;&#x27;.format(9)) # 转成八进制，结果为：11print(&#x27;&#123;0:x&#125;&#x27;.format(15)) # 转成十六进制，结果为：fprint(&#x27;&#123;0:,&#125;&#x27;.format(99812939393931)) # 千分位格式化，结果为：99,812,939,393,931# 2.3 f:python3.5以后才推出x = input(&#x27;your name: &#x27;)y = input(&#x27;your age: &#x27;)res = f&#x27;我的名字是&#123;x&#125; 我的年龄是&#123;y&#125;&#x27;# raw字符串：取消所有的转义r&#x27;\\n&#x27;表示2个字符&#x27;\\&#x27;和&#x27;n&#x27;,而不是换行 r&#x27;&#123;&#125;&#x27;.format(src_file) #raw只针对转义，不针对&#123;&#125;，format方法还是可以使用的 常用方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990按索引取值(正向取+反向取) ：只能取，不能改，因为str是常量msg=&#x27;hello world&#x27;# 正向取# print(msg[0])# 反向取# print(msg[-1])#切片.相当于浅拷贝copy，只拷贝顶层引用# res=msg[0:5]# 步长msg = &#x27;hello world&#x27;# res=msg[0:5:2] # 0 2 4# 反向步长# res=msg[5:0:-1] #5 4 3 2 1# print(res) #&quot; olle&quot;移除字符串左右两侧的符号strip、lstrip、rstrip# 默认去掉的空格# msg=&#x27; czw &#x27;# res=msg.strip()# print(msg) # 不会改变原值# print(res) # 是产生了新值# 可指定移除的字符# msg=&#x27;****czw****&#x27;# print(msg.strip(&#x27;*&#x27;))切分split：把一个字符串按照某种分隔符进行切分，得到一个列表。rsplit# # 默认分隔符是空格# info=&#x27;czw 18 male&#x27;# res=info.split()# # 指定分隔符# info=&#x27;czw:18:male&#x27;# res=info.split(&#x27;:&#x27;)# 指定分隔次数(了解)# info=&#x27;czw:18:male&#x27;# res=info.split(&#x27;:&#x27;,1)lower,upper# msg=&#x27;AbbbCCCC&#x27;# print(msg.lower())# print(msg.upper())startswith,endswith# print(&quot;alex is sb&quot;.startswith(&quot;alex&quot;))# print(&quot;alex is sb&quot;.endswith(&#x27;sb&#x27;))join: 把可迭代对象里的字符串元素拼接成字符串# l=[&#x27;czw&#x27;, &#x27;18&#x27;, &#x27;male&#x27;]# res=&quot;:&quot;.join(l) # 按照某个分隔符号，把元素全为字符串的列表拼接成一个大字符串replace# msg=&quot;you can you up no can no bb&quot;# print(msg.replace(&quot;you&quot;,&quot;YOU&quot;,))# print(msg.replace(&quot;you&quot;,&quot;YOU&quot;,1))isdigit# 判断字符串是否由整数组成# print(&#x27;123&#x27;.isdigit()) True# print(&#x27;12.3&#x27;.isdigit()) Falsefind,rfind,index,rindexmsg=&#x27;hello czw hahaha&#x27;# 找到返回起始索引# print(msg.find(&#x27;e&#x27;)) # 返回要查找的字符串在大字符串中的起始索引# print(msg.find(&#x27;czw&#x27;))# print(msg.index(&#x27;e&#x27;))# print(msg.index(&#x27;czw&#x27;))# 找不到# print(msg.find(&#x27;xxx&#x27;)) # 返回-1，代表找不到# print(msg.index(&#x27;xxx&#x27;)) # 抛出异常count# msg=&#x27;hello czw hahaha czw、 czw&#x27;# print(msg.count(&#x27;czw&#x27;))is系列# print(&#x27;abc&#x27;.islower())# print(&#x27;ABC&#x27;.isupper())# print(&#x27;123123aadsf&#x27;.isalnum()) # 字符串由字母或数字组成结果为True# print(&#x27;ad&#x27;.isalpha()) # 字符串由由字母组成结果为True# print(&#x27; &#x27;.isspace()) # 字符串由空格组成结果为True 列表常用方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950追加# l=[111,&#x27;czw&#x27;,&#x27;hello&#x27;]# l.append(3333)# l.append(4444)# print(l)插入值# l=[111,&#x27;czw&#x27;,&#x27;hello&#x27;]# l.insert(0,&#x27;alex&#x27;)# print(l)extend拓展列表# new_l=[1,2,3]# l=[111,&#x27;czw&#x27;,&#x27;hello&#x27;]# l.extend(new_l)# print(l) [111, &#x27;czw&#x27;, &#x27;hello&#x27;, 1, 2, 3]删除# 方式一：通用的删除方法，只是单纯的删除、没有返回值# l = [111, &#x27;czw&#x27;, &#x27;hello&#x27;]# del l[1]# 方式二：l.pop()根据索引删除，会返回删除的值# l = [111, &#x27;czw&#x27;, &#x27;hello&#x27;]# l.pop() # 不指定索引默认删除最后一个# print(l)# 方式三：l.remove()根据元素删除，返回None# l = [111, &#x27;czw&#x27;, [1,2,3],&#x27;hello&#x27;]# l.remove([1,2,3])需要掌握操作l = [1, &#x27;aaa&#x27;, &#x27;bbb&#x27;,&#x27;aaa&#x27;,&#x27;aaa&#x27;]#l.count(&#x27;aaa&#x27;)# print(l.index(&#x27;aaa&#x27;))# print(l.index(&#x27;aaaaaaaaa&#x27;)) # 找不到报错# l.clear()# l.reverse():不是排序，就是将列表倒过来# l.sort(): 列表内元素必须是同种类型才可以排序# l=[11,-3,9,2,3.1]# l.sort() # 默认从小到大排，称之为升序# l.sort(reverse=True) # 从大到小排，设置为降序 元组常用方法1234567按照索引/位置存放多个值，只用于读不用于改。如果元组中只有一个元素，必须加逗号索引取值t.index(ele)# 获得指定元素的索引t.count(ele)# 获得指定元素在元组中的个数 字典常用方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748value可以是任意类型，但是key必须是不可变类型,且不能重复造字典的方式一：# d=&#123;&#x27;k1&#x27;:111, (1,2,3):222&#125;造字典的方式二：# d=dict(x=1,y=2,z=3)造字典的方式三：# info=[# [&#x27;name&#x27;,&#x27;czw&#x27;],# (&#x27;age&#x27;,18),# [&#x27;gender&#x27;,&#x27;male&#x27;]# ]# res=dict(info)按key存取值：可存可取# d=&#123;&#x27;k1&#x27;:111&#125;# 针对赋值操作：key存在，则修改# d[&#x27;k1&#x27;]=222# 针对赋值操作：key不存在，则创建新值# d[&#x27;k2&#x27;]=3333成员运算in和not in:根据key# d=&#123;&#x27;k1&#x27;:111,&#x27;k2&#x27;:2222&#125;# print(&#x27;k1&#x27; in d)# print(111 in d)#4、删除d=&#123;&#x27;k1&#x27;:111,&#x27;k2&#x27;:2222&#125;# 4.1 通用删除# del d[&#x27;k1&#x27;]# 4.2 pop删除：根据key删除元素，返回删除key对应的那个value值# res=d.pop(&#x27;k2&#x27;)# 4.3 popitem删除：随机删除，返回元组(删除的key,删除的value)# res=d.popitem()键keys()，值values()，键值对items()d=&#123;&#x27;k1&#x27;:111&#125;#1、d.clear()#2、d.update()：根据参数扩展字典，如果原字典无则添加；如原字典有，若一样则不变，不一样则修改d.update(&#123;&#x27;k2&#x27;:222,&#x27;k3&#x27;:333&#125;) #&#123;&#x27;k1&#x27;: 111, &#x27;k2&#x27;: 222, &#x27;k3&#x27;: 333&#125;print(d)#3、d.get() ：根据key取值，容错性好# print(d[&#x27;k2&#x27;]) # key不存在则报错 集合常用方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960在&#123;&#125;内用逗号分隔开多个元素，多个元素满足以下三个条件# 1. 集合内元素必须为不可变类型# 2. 集合内元素无序# 3. 集合内元素没有重复# 4.1 取交集：两者共同的好友# res=friends1 &amp; friends2# print(res)# print(friends1.intersection(friends2))# 4.2 取并集/合集：两者所有的好友# print(friends1 | friends2)# print(friends1.union(friends2))# 4.3 取差集：取friends1独有的好友# print(friends1 - friends2)# print(friends1.difference(friends2))# 取friends2独有的好友# print(friends2 - friends1)# print(friends2.difference(friends1))# 4.4 对称差集: 求两个用户独有的好友们（即去掉共有的好友）# print(friends1 ^ friends2)# print(friends1.symmetric_difference(friends2))# 4.5 父子集：包含的关系# s1=&#123;1,2,3&#125;# s2=&#123;1,2,4&#125;# 不存在包含关系，下面比较均为False# print(s1 &gt; s2)# print(s1 &lt; s2)# s1=&#123;1,2,3&#125;# s2=&#123;1,2&#125;# print(s1 &gt; s2) # 当s1大于或等于s2时，才能说是s1是s2他爹# =========================去重=========================# 1、只能针对不可变类型去重,即可迭代对象中的元素为不可变类型# print(set([1,1,1,1,2])) ==》｛1,2｝# print(set([[1, ],1,1,1,2])) ==》error,因为[1, ]不是不可变类型# 2、无法保证原来的顺序# l=[1,&#x27;a&#x27;,&#x27;b&#x27;,&#x27;z&#x27;,1,1,1,2]# l=list(set(l))# 需要掌握的内置方法1：discard# s.discard(4) # 删除元素不存在do nothing# s.remove(4) # 删除元素不存在则报错# 需要掌握的内置方法2：update：与字典的update的用法一致# s.update(&#123;1,3,5&#125;)# 需要掌握的内置方法3：pop：随机弹出一个值，返回弹出的值# res=s.pop()# 需要掌握的内置方法4：add：添加一个值# s.add(4)","categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"python","slug":"code/python","permalink":"http://example.com/categories/code/python/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"Python杂记","slug":"python基础","date":"2023-02-05T06:07:16.750Z","updated":"2023-02-05T07:06:46.755Z","comments":true,"path":"2023/02/05/python基础/","link":"","permalink":"http://example.com/2023/02/05/python%E5%9F%BA%E7%A1%80/","excerpt":"","text":"类 类中属性查找的顺序：对象&#x3D;&gt;类&#x3D;&gt;父类 闭包函数闭包函数时一种广泛的叫法，存在于所有的编程语言中，只要符合以下2点就是闭包函数： 闭包函数定义在其他函数的内部 闭包函数对外部作用域有引用 123456789101112131415161718装饰器就是闭包函数的一个典型应用：def wrapper(func): def inner(*args, **kwargs): # code res = func(*args, **kwargs) # code return res return innerdef a(): print(hello)a = wrapper(a)# 把b当作参数传入wrapper，并把返回值赋给b@wrapperdef b(): print(hello) pymysql的使用1234567891011121314151617181920212223242526import pymysql#连接数据库conn=pymysql.connect(host=&#x27;101.133.225.166&#x27;, user=&#x27;root&#x27;, password=&quot;123456&quot;,database=&#x27;test&#x27;, port=3306)# 获取游标cursor=conn.cursor(cursor=pymysql.cursors.DictCursor) # 查出来数据是字典格式# 操作 定义一个sql# sql=&#x27;select id,name from book&#x27;# cursor.execute(sql)# ret=cursor.fetchall()# print(ret)# 插入# sql=&#x27;insert into book(id,name) values (%s,%s)&#x27;# cursor.execute(sql,[3,&#x27;lqz&#x27;])# conn.commit()# 删除# sql=&#x27;delete from book where name=%s&#x27;# cursor.execute(sql,[&#x27;lqz&#x27;])# conn.commit()# 更新# sql=&#x27;update book set name=%s where id=%s&#x27;# cursor.execute(sql,[&#x27;xxx&#x27;,1])# conn.commit() 字典传参1**dict传值，可以等同于**kwargs，将字典打散按关键字传值 小技巧lambda表达式匿名函数lambda用于临时调用一次的场景：更多的是将匿名函数与其他函数配合使用 lambda 参数: 返回值 12345678910111213141516171819202122232425应用场景：某些函数里需要一些经过简单处理后的数据，那么这个简单处理的过程就可以用lambda更为简便地实现。比如max(iterable, *, key=None)函数中，key实参指定排序函数用的参数。返回可迭代对象中最大的元素salaries=&#123; &#x27;siry&#x27;:3000, &#x27;tom&#x27;:7000, &#x27;lili&#x27;:10000, &#x27;jack&#x27;:2000&#125;# 迭代出的内容 比较的值# &#x27;siry&#x27; 3000# &#x27;tom&#x27; 7000# &#x27;lili&#x27; 10000# &#x27;jack&#x27; 2000问题就是key默认是迭代出的内容，但我们希望key是salaries[迭代出的内容]，即薪水的资料，这时就可以用lambda简单的处理一下# def func(k):# return salaries[k]# res=max(salaries,key=func) # 返回值=func(&#x27;siry&#x27;) #这是key需要的就是salaries[迭代的内容]这样处理过的数据。这里func会自动以迭代出的内容作为参数执行，再把返回值赋给key# res=max(salaries,key=lambda k:salaries[k]) 这时lambda等同于func，但明显func还要单独定义啥的，比较复杂，不如lambda简单。这里的k默认就是迭代出的内容。注意，这里返回的res是满足条件的可迭代对象，而不是最大的比较的值还可应用于列表排序：a = [(1, 2), (4, 1), (9, 10), (13, -3)]a.sort(key=lambda x: x[1]) 三元表达式12expression1 if condition else expression2x = 1 if 1 &gt; 3 else 3 生成式1234567891011121、列表推导式 [i*2 for i in [1, 2]]2、字典生成式# items=[(&#x27;name&#x27;,&#x27;egon&#x27;),(&#x27;age&#x27;,18),(&#x27;gender&#x27;,&#x27;male&#x27;)]# res=&#123;k:v for k,v in items if k != &#x27;gender&#x27;&#125;3、集合生成式# keys=[&#x27;name&#x27;,&#x27;age&#x27;,&#x27;gender&#x27;]# set1=&#123;key for key in keys&#125;4、生成器表达式# g=(i for i in range(10) if i &gt; 3) yield解析12345678910111213141516171819202122232425262728293031323334353637383940&#x27;&#x27;&#x27;yield函数解析：yield出现在函数中会使函数的调用结果变为一个生成器.这样的函数可以暂时挂起，保存运行状态.相当于在yield那一句弄了个断点在函数中，yield的作用有俩： 1.赋值功能：外界send(arg),把当前的yield赋值为arg，接着继续运行函数 2.返回结果功能：函数运行到新的yield时返回此时yield后的值&#x27;&#x27;&#x27;def test(): print(&#x27;before&#x27;) while True: x = yield print(x)# &#x27;赋值功能演示&#x27;# g1 = test()#因为yield函数的调用结果变为一个生成器# next(g1)#等同于g1.send(none),初始化生成器，使生成器停止在x=yield那一步（这一步还没运行完成，只会先返回yield后的值，相当于是一个初始状态了）# g1.send(&#x27;hahaha&#x27;)#函数接着上一步运行x=yield,send的功能是把参数传给yield。然后函数本身又执行x=yield的赋值操作，所以最后# #对x的打印结果就是传进去的参数。接着函数运行到新的x=yield(这一句还没运行)时因遇到新的yield而暂时挂起。等待下一次对这个生成器的调用# g1.send(&#x27;xixixi&#x27;)&#x27;返回值功能演示&#x27;def test2(): food_list = [] while True: food = yield food_list food_list.append(food) print(f&#x27;此次传参：&#123;food&#125;&#x27;)g2 = test2()g2.send(None)#初始化生成器,停在food = yield food_list(这一步还没运行完成，只会先返回yield后的值，即返回[],相当于是一个初始状态了)print(g2.send(&#x27;apple&#x27;))#首先传值，再继续运行函数，直到遇到新的yield，返回yield后面跟着的food_list。 #之后函数再继续挂起，下一次运行时，函数的状态就是现在的状态。。 # 注意：顺序是传值-运行函数-返回结果，遇到yield就挂起函数print(g2.send(&#x27;banana&#x27;))print(g2.send([&#x27;apple&#x27;, &#x27;banana&#x27;])) 反射机制1234567891011121314151617181920212223# 本质就是通过字符串操作对象.出来的属性# 实现反射机制的步骤# 1、先通过dir：查看出某一个对象下可以.出哪些属性来# print(dir(obj))# 2、可以通过字符串反射到真正的属性上，得到属性值# print(obj.__dict__[dir(obj)[-2]]) 对象的属性值都存在__dict__这个字典里# 四个内置函数的使用:通过字符串来操作属性值# 1、hasattr()# print(hasattr(obj,&#x27;name&#x27;))# print(hasattr(obj,&#x27;x&#x27;))# 2、getattr()# print(getattr(obj,&#x27;name&#x27;))# 3、setattr()# setattr(obj,&#x27;name&#x27;,&#x27;EGON&#x27;) # obj.name=&#x27;EGON&#x27;# print(obj.name)# 4、delattr()# delattr(obj,&#x27;name&#x27;) # del obj.name# print(obj.__dict__)","categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"python","slug":"code/python","permalink":"http://example.com/categories/code/python/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"知识分享","slug":"知识分享","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"}]},{"title":"hexo搭建","slug":"hexo笔记","date":"2023-02-03T07:44:20.000Z","updated":"2023-02-04T01:46:56.386Z","comments":true,"path":"2023/02/03/hexo笔记/","link":"","permalink":"http://example.com/2023/02/03/hexo%E7%AC%94%E8%AE%B0/","excerpt":"","text":"hexo搭建参考博客：hexo博客搭建即可。写得挺好，但是有些地方没有说明白，在下面补充一下。 配置hexo补充说明 虽然启动的时候在本地是http://localhost:4000.com,,但是你用https://username.github.io也能访问 clone下来的主题要放在hexo根目录下的themes文件夹中才有效。否则会出现读取文件时读到null对象的情况 clone下来的文件夹的名字要改为主题名字，比如clone下来的文件夹是hexo-theme-nexmoe，那么就要改成nexmoe。否则会出现index.html no layout的报错 source资源文件夹： 是存放用户资源的地方。除 _posts 文件夹之外，开头命名为 _ (下划线)的文件 &#x2F; 文件夹和隐藏的文件将会被忽略。Markdown(会被解析成html) 和 HTML 文件会被解析并放到 public 文件夹，而其他文件会被拷贝过去。而public文件夹就相当于服务器向外界中开放的资源，里面的内容可以通过/文件名进行访问。 特别的，如果你要开放一张图片的资源(以微信二维码为例)，那么首先你要把图片下载到source文件夹中，再在.yml配置文件中(或其他途径)将前端相应的链接改为/winxin.jpg,那样就可以通过前端页面访问到图片资源了。 网站的根目录就是source文件夹。所以在编写.md文件时，如果要应用图片的话，图片路径以/开头的话就是从source目录开始找文件 主题的配置文件要在对应的主题根目录下的_config.yml修改才有效。其中配置文件中的/文件名就是访问的source中的文件。如果修改了不生效，可以先把hexo根目录下的对应主题的配置文件删了，再用hexo g命令重新生成一份就好了","categories":[{"name":"教程","slug":"教程","permalink":"http://example.com/categories/%E6%95%99%E7%A8%8B/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-02-01T01:45:59.976Z","updated":"2023-02-04T01:19:08.967Z","comments":true,"path":"2023/02/01/hello-world/","link":"","permalink":"http://example.com/2023/02/01/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"test","slug":"test","permalink":"http://example.com/categories/test/"}],"tags":[{"name":"system","slug":"system","permalink":"http://example.com/tags/system/"},{"name":"Games","slug":"Games","permalink":"http://example.com/tags/Games/"}]}],"categories":[{"name":"code","slug":"code","permalink":"http://example.com/categories/code/"},{"name":"python","slug":"code/python","permalink":"http://example.com/categories/code/python/"},{"name":"爬虫","slug":"code/python/爬虫","permalink":"http://example.com/categories/code/python/%E7%88%AC%E8%99%AB/"},{"name":"django","slug":"code/python/django","permalink":"http://example.com/categories/code/python/django/"},{"name":"windows","slug":"code/windows","permalink":"http://example.com/categories/code/windows/"},{"name":"教程","slug":"教程","permalink":"http://example.com/categories/%E6%95%99%E7%A8%8B/"},{"name":"test","slug":"test","permalink":"http://example.com/categories/test/"}],"tags":[{"name":"code","slug":"code","permalink":"http://example.com/tags/code/"},{"name":"知识分享","slug":"知识分享","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"system","slug":"system","permalink":"http://example.com/tags/system/"},{"name":"Games","slug":"Games","permalink":"http://example.com/tags/Games/"}]}